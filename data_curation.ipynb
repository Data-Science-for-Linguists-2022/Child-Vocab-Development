{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0361b10a-3d8d-4fdd-876a-dd275328f930",
   "metadata": {},
   "source": [
    "\n",
    "# Data curation\n",
    "\n",
    "Man Ho Wong | m.wong@pitt.edu | Feb 27th, 2022\n",
    "\n",
    "This notebook search for the datasets needed for this project in the following database:\n",
    "- [CHILDES](https://childes.talkbank.org/)  \n",
    "  *Reference:* MacWhinney, B. (2000). The CHILDES Project: Tools for analyzing talk. Third Edition. Mahwah, NJ: Lawrence Erlbaum Associates.\n",
    "\n",
    "I may not need datasets from [Wordbank](http://wordbank.stanford.edu/) as I found that CHILDES probably has all the data I need.\n",
    "\n",
    "I will also explore the datasets on the way to get a sense of the contents and the structures of the datasets (such as participant information, annotations, data format, etc.), as well as some basic statistics about the datasets. After that, I will identify the information I need in the datasets and compile the data for data processing later.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2624715b-d6a5-41f6-b7bc-adf57127d855",
   "metadata": {},
   "source": [
    "\n",
    "# 1 Searching for suitable corpora in CHILDES\n",
    "\n",
    "CHILDES is a multilingual database containing corpora with transcriptions, audio recordings and/or video recordings of child speech and child-directed speech (CDS) at different developmental stages. Each corpus has a separate directory for each participant, and each directory contains the recording transcripts stored in CHAT formats. ([Example](https://childes.talkbank.org/access/Eng-NA/Brown.html))\n",
    "\n",
    "For this project, I will need to collect the transcipts for both the child speech and the associated CDS. Additionally, I will need the participant information (i.e. child age, sex and socioeconomic status (SES), mother's education) and some basic annotations of the words (i.e. morphemes and lexical categories). Participant information can be found in the header of each CHAT file as the metadata of the file. Annotation information can be found as dependent tiers embedded in the transcription.\n",
    "\n",
    "I will first search for the datasets meeting the needs of the project. Here are the basic search criteria:\n",
    "- Language: North American English (Note: I may need other languages later for comparison)\n",
    "- Participants: contains only child and mother (i.e. other people such as investigators were not involved)\n",
    "- Child information: contains child age, sex and socioeconomic status (SES)\n",
    "- Mother information: contains socioeconomic status (SES), education\n",
    "- Annotation: contains morpheme and/or grammatical tiers\n",
    "\n",
    "Let's take a quick look at a sample CHAT file first to see how the data is organized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98686cc-77f7-4f43-91dd-4575e7261f98",
   "metadata": {},
   "source": [
    "\n",
    "## 1.1 Reading CHAT file\n",
    "\n",
    "The `PyLangAcq` package allows users to read CHAT files directly from a zip file. You can download and install it with the following code:  \n",
    "`$ pip install --upgrade pylangacq`\n",
    "\n",
    "For documentation, you can visit their [website](https://pylangacq.org/).\n",
    "\n",
    "I will use the Brown Corpus of CHILDES as an example below. The corpus has been downloaded from [here](https://childes.talkbank.org/data/Eng-NA/Brown.zip) and stored under `data_samples/childes/Brown.zip`. There are three folders in this corpus, each folder contains a dataset (a collection of CHAT files) for each child:\n",
    "\n",
    "```\n",
    "Brown.zip/  \n",
    "    |--Adam/  \n",
    "    |--Eve/  \n",
    "    |--Sarah/\n",
    "```\n",
    "\n",
    "I will use the `read_chat()` function of `PyLangAcq` to read all the CHAT files in the dataset `Adam`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce9e997b-a6eb-4ede-b8d2-eba6cc55bab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pylangacq.chat.Reader'>\n",
      "Number of CHAT files: 55\n"
     ]
    }
   ],
   "source": [
    "import pylangacq\n",
    "\n",
    "# Read CHAT files in the dataset 'Adam' in 'Brown.zip':\n",
    "path = 'data_samples/childes/Brown.zip'\n",
    "adam = pylangacq.read_chat(path, 'Adam')\n",
    "\n",
    "print(type(adam))\n",
    "print('Number of CHAT files:', adam.n_files())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eda0575c-87d5-458f-915e-47a7737f4bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ages (year, month, day): [(2, 3, 4), (2, 3, 18), (2, 4, 3), (2, 4, 15), (2, 4, 30), (2, 5, 12), (2, 6, 3), (2, 6, 17), (2, 7, 1), (2, 7, 14), (2, 8, 1), (2, 8, 16), (2, 9, 4), (2, 9, 18), (2, 10, 2), (2, 10, 16), (2, 10, 30), (2, 11, 13), (2, 11, 28), (3, 0, 11), (3, 0, 25), (3, 1, 9), (3, 1, 26), (3, 2, 9), (3, 2, 21), (3, 3, 4), (3, 3, 18), (3, 4, 1), (3, 4, 18), (3, 5, 1), (3, 5, 15), (3, 5, 29), (3, 6, 9), (3, 7, 7), (3, 8, 1), (3, 8, 14), (3, 8, 26), (3, 9, 16), (3, 10, 15), (3, 11, 1), (3, 11, 14), (4, 0, 14), (4, 1, 15), (4, 2, 17), (4, 3, 9), (4, 4, 1), (4, 4, 13), (4, 5, 11), (4, 6, 24), (4, 7, 1), (4, 7, 29), (4, 9, 2), (4, 10, 2), (4, 10, 23), (5, 2, 12)]\n"
     ]
    }
   ],
   "source": [
    "# Ages when recordings were made\n",
    "print('Ages (year, month, day):', adam.ages())  # output: a list of tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4f47d0-163c-498c-81a2-7828c9270d7d",
   "metadata": {},
   "source": [
    "As shown above, `read_chat()` read the CHAT files and creates a `Reader` object. This is a `dataclass` storing data and metadata across all the CHAT files in `Adam`. You can access the data stored in the `Reader` by calling the appropriate methods, such as `.n_files()` for number of CHAT files in the dataset. For example, `Adam` has 55 CHAT files. We can also get the ages when recordings were made by calling `.ages()`. Let's see what other information we can get from the `Reader` object in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb97d197-845e-46e4-a494-cd3765572cb0",
   "metadata": {},
   "source": [
    "## 1.2 Accessing metadata stored in a CHAT file\n",
    "\n",
    "Metadata such as age range, date of recording, participants, etc. are stored in the header of each CHAT file. We can access these information by calling the `.header()` method. Here is the header for the first CHAT file in `adam`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caad807e-c699-4e50-9dfc-f9663e6c8f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UTF8': '',\n",
       " 'PID': '11312/c-00015632-1',\n",
       " 'Languages': ['eng'],\n",
       " 'Participants': {'CHI': {'name': 'Adam',\n",
       "   'language': 'eng',\n",
       "   'corpus': 'Brown',\n",
       "   'age': '2;03.04',\n",
       "   'sex': 'male',\n",
       "   'group': 'TD',\n",
       "   'ses': 'MC',\n",
       "   'role': 'Target_Child',\n",
       "   'education': '',\n",
       "   'custom': ''},\n",
       "  'MOT': {'name': 'Mother',\n",
       "   'language': 'eng',\n",
       "   'corpus': 'Brown',\n",
       "   'age': '',\n",
       "   'sex': 'female',\n",
       "   'group': '',\n",
       "   'ses': '',\n",
       "   'role': 'Mother',\n",
       "   'education': '',\n",
       "   'custom': ''},\n",
       "  'URS': {'name': 'Ursula_Bellugi',\n",
       "   'language': 'eng',\n",
       "   'corpus': 'Brown',\n",
       "   'age': '',\n",
       "   'sex': '',\n",
       "   'group': '',\n",
       "   'ses': '',\n",
       "   'role': 'Investigator',\n",
       "   'education': '',\n",
       "   'custom': ''},\n",
       "  'RIC': {'name': 'Richard_Cromer',\n",
       "   'language': 'eng',\n",
       "   'corpus': 'Brown',\n",
       "   'age': '',\n",
       "   'sex': '',\n",
       "   'group': '',\n",
       "   'ses': '',\n",
       "   'role': 'Investigator',\n",
       "   'education': '',\n",
       "   'custom': ''},\n",
       "  'COL': {'name': 'Colin_Fraser',\n",
       "   'language': 'eng',\n",
       "   'corpus': 'Brown',\n",
       "   'age': '',\n",
       "   'sex': '',\n",
       "   'group': '',\n",
       "   'ses': '',\n",
       "   'role': 'Investigator',\n",
       "   'education': '',\n",
       "   'custom': ''}},\n",
       " 'Date': {datetime.date(1962, 10, 8), datetime.date(1962, 10, 9)},\n",
       " 'Comment': 'Birth of CHI is 4-JUL-1960',\n",
       " 'Time Duration': '15:00-16:00',\n",
       " 'Types': 'long, toyplay, TD'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam.headers()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29beeb4-d948-4c36-b2f3-eab0a9095ffb",
   "metadata": {},
   "source": [
    "The output above, is a multilevel `dictionary`. TO retrieve a specific piece of information we need, we can use the `dictionary` keys as usual.  \n",
    "Let's check if 'Adam' is a male as its biblical name suggests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4850e46-5e97-4e20-8a06-6aea94ef4090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam.headers()[0]['Participants']['CHI']['sex']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad9708f-391c-4edf-a245-25e4c2d06095",
   "metadata": {},
   "source": [
    "\n",
    "## 1.3 Accessing annotations\n",
    "\n",
    "Next, I will check what kinds of annotation information are stored in each CHAT file. I will use the `.tokens()` method to access the tokens with annotation information. This method creates a `list` of `Token` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d0375c-caed-42a4-9afc-d121b3f79e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(word='play', pos='n', mor='play', gra=Gra(dep=1, head=2, rel='MOD')),\n",
       " Token(word='checkers', pos='n', mor='checker-PL', gra=Gra(dep=2, head=0, rel='INCROOT')),\n",
       " Token(word='.', pos='.', mor='', gra=Gra(dep=3, head=2, rel='PUNCT')),\n",
       " Token(word='big', pos='adj', mor='big', gra=Gra(dep=1, head=2, rel='MOD')),\n",
       " Token(word='drum', pos='n', mor='drum', gra=Gra(dep=2, head=0, rel='INCROOT'))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = adam.tokens()\n",
    "tokens[:5]  # first five tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f07287-85f1-45ef-8234-a647f9a4fd24",
   "metadata": {},
   "source": [
    "Each `Token` is a `dataclass` with attributes (e.g. `word`,`pos`, etc.) as shown in the above example.  \n",
    "Annotations for each word are stored as the `Token`'s attributes (i.e. attributes other than `word`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2bf4834-9464-4b45-9c7e-3d8074646518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second token in 'Adam':\n",
      "Word: checkers\n",
      "Morpheme: checker-PL\n",
      "Part of speech: n\n"
     ]
    }
   ],
   "source": [
    "print(\"Second token in 'Adam':\")\n",
    "print('Word: {}\\nMorpheme: {}\\nPart of speech: {}'.format(\n",
    "    tokens[1].word, tokens[1].mor, tokens[1].pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a51c99-28b0-4d98-97ec-8a61817ab671",
   "metadata": {},
   "source": [
    "\n",
    "## 1.4 Searching for suitable corpora\n",
    "\n",
    "Now that we know what kinds of information are stored in each dataset and how we can access them, we can start searching for the corpora for the project according to the criteria set previously.\n",
    "\n",
    "There are dozens of English corpora in CHILDES. We don't need to download them all at once just to look for the corpora we need. `PyLangAcq` allows user to read a corpus directly with the corpus's URL. We can read the corpora one by one, and keep only the ones we need. To get the URLs for all the North American English corpora, one can use some web scraping tools to get all the links from the database's website and look for the corpus URLs from there. However, a much faster way is to take advantage of the TalkBank's [browsable database](https://sla.talkbank.org/TBB/childes):  \n",
    "1. navigate to CHILDES's North American English datasets (Eng-NA)\n",
    "2. copy the list of corpora directly to a spreadsheet program and save it as a `csv` file (example: `data_samples/childes/eng_NA_corpus_list.csv`)\n",
    "3. construct the URL for each corpus simply from the name of the corpus we get from step 2 (see below)\n",
    "\n",
    "CHILDES has a very well organized structure. Each corpus has the same URL format as follow:  \n",
    "`https://childes.talkbank.org/data/LANGUAGE/NAME_OF_CORPUS.zip`  \n",
    "For example, the URL for the Brown Corpus is: https://childes.talkbank.org/data/Eng-NA/Brown.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7132000-53ab-4b6b-b187-4f647de52636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 47 NA English corpora in CHILDES.\n",
      "Here are the first 10 corpora:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        Bates\n",
       "1    Bernstein\n",
       "2        Bliss\n",
       "3        Bloom\n",
       "4     Bohannon\n",
       "5    Braunwald\n",
       "6        Brent\n",
       "7        Brown\n",
       "8        Clark\n",
       "9    Demetras1\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the list of corpora into a Pandas Series:\n",
    "corpus_list = pd.read_csv('data/childes/eng_NA_corpus_list.csv', \n",
    "                          header=None, index_col=False, squeeze = True)\n",
    "\n",
    "print('There are {} NA English corpora in CHILDES.'.format(len(corpus_list)))\n",
    "print('Here are the first 10 corpora:')\n",
    "corpus_list.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d611aa-aa63-4fac-8306-a8e25464cd0a",
   "metadata": {},
   "source": [
    "There are 47 corpora in CHILDES that we can potentially use! Let's try to look for the corpora matching some of the criteria set previously, e.g. corpora containing information about child's/ mother's SES and educational background. As shown previously, we can use `.headers` to access these information in the CHAT files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a43cb57-1335-432f-9ba8-bbb473cf9de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Bates...\n",
      "Bates matches the criteria!\n",
      "\n",
      "Searching Bernstein...\n",
      "Bernstein matches the criteria!\n",
      "\n",
      "Searching Bliss...\n",
      "Searching Bloom...\n",
      "Searching Bohannon...\n",
      "Searching Braunwald...\n",
      "Searching Brent...\n",
      "Searching Brown...\n",
      "Brown matches the criteria!\n",
      "\n",
      "Searching Clark...\n",
      "Clark matches the criteria!\n",
      "\n",
      "Searching Demetras1...\n",
      "Searching Demetras2...\n",
      "Demetras2 matches the criteria!\n",
      "\n",
      "Searching Evans...\n",
      "Searching Feldman...\n",
      "Searching Garvey...\n",
      "Searching Gathercole...\n",
      "Searching Gelman...\n",
      "Searching Gleason...\n",
      "Gleason matches the criteria!\n",
      "\n",
      "Searching Gopnik...\n",
      "Searching HSLLD...\n",
      "HSLLD matches the criteria!\n",
      "\n",
      "Searching Haggerty...\n",
      "Searching Hall...\n",
      "Hall matches the criteria!\n",
      "\n",
      "Searching Hicks...\n",
      "Searching Higginson...\n",
      "Searching Kuczaj...\n",
      "Searching MacWhinney...\n",
      "Searching McCune...\n",
      "Searching McMillan...\n",
      "Searching Morisset...\n",
      "Searching Nelson...\n",
      "Searching NewEngland...\n",
      "Searching NewmanRatner...\n",
      "NewmanRatner matches the criteria!\n",
      "\n",
      "Searching Peters...\n",
      "Searching PetersonMcCabe...\n",
      "Searching Post...\n",
      "Post matches the criteria!\n",
      "\n",
      "Searching Rollins...\n",
      "Searching Sachs...\n",
      "Searching Sawyer...\n",
      "Searching Snow...\n",
      "Searching Soderstrom...\n",
      "Searching Sprott...\n",
      "Searching Suppes...\n",
      "Searching Tardif...\n",
      "Searching Valian...\n",
      "Searching VanHouten...\n",
      "VanHouten matches the criteria!\n",
      "\n",
      "Searching VanKleeck...\n",
      "Searching Warren...\n",
      "Searching Weist...\n"
     ]
    }
   ],
   "source": [
    "search_result = []  # To store a list of corpora matching the criteria\n",
    "\n",
    "# Search each corpus in the list:\n",
    "for corpus_name in corpus_list:\n",
    "    \n",
    "    # Construct the URL for the corpus from its name:\n",
    "    corpus_url = 'https://childes.talkbank.org/data/Eng-NA/'+corpus_name+'.zip'\n",
    "    \n",
    "    # read the corpus into a Reader object:\n",
    "    print('Searching {}...'.format(corpus_name))\n",
    "    corpus = pylangacq.read_chat(corpus_url)\n",
    "    \n",
    "    # Inspect each CHAT file in the corpus:\n",
    "    found = False    \n",
    "    for file in range(corpus.n_files()):\n",
    "        \n",
    "        # Search criteria:\n",
    "        # dataset must contain only child ('CHI') and mother (MOT) as participants\n",
    "        # (as we are looking at child-directed speech). \n",
    "        # dataset must include child's or mother's SES/education.\n",
    "        if (\n",
    "            (('CHI' in corpus.headers()[file]['Participants']) and \n",
    "            ('MOT' in corpus.headers()[file]['Participants']) and\n",
    "            ('INV' not in corpus.headers()[file]['Participants'])) \n",
    "            \n",
    "            and\n",
    "            \n",
    "            ((corpus.headers()[file]['Participants']['MOT']['ses'] != '') or\n",
    "             (corpus.headers()[file]['Participants']['CHI']['ses'] != '') or\n",
    "             (corpus.headers()[file]['Participants']['MOT']['education'] != ''))\n",
    "        ):\n",
    "            \n",
    "            print('{} matches the criteria!\\n'.format(corpus_name))\n",
    "            search_result.append(corpus_name) \n",
    "            found = True\n",
    "            \n",
    "        # Break the for loop if criteria are matched and move on to another\n",
    "        # corpus. (If one CHAT file contains the needed information, I will keep\n",
    "        # the whole corpus for data processing later and remove individual CHAT\n",
    "        # files missing the needed information.)\n",
    "        if found == True:\n",
    "            break\n",
    "\n",
    "print('Search completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ab9831-c39c-42eb-a9eb-5ce182450676",
   "metadata": {},
   "source": [
    "Let's see which corpora contain the data we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a281639-489c-4578-86dc-6a779ac693e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11 corpora matching the criteria:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Bates',\n",
       " 'Bernstein',\n",
       " 'Brown',\n",
       " 'Clark',\n",
       " 'Demetras2',\n",
       " 'Gleason',\n",
       " 'HSLLD',\n",
       " 'Hall',\n",
       " 'NewmanRatner',\n",
       " 'Post',\n",
       " 'VanHouten']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\n{} corpora matching the criteria:'.format(len(search_result)))\n",
    "search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90501fb-2470-4586-9544-7c5efcbdff7e",
   "metadata": {},
   "source": [
    "Nice! We have narrowed down the number of corpora we need to process from 47 to 11.\n",
    "Next, I will create a list of `Reader` objects each contains all the CHAT files in each corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32a162ad-8e22-4847-8a83-3a8b75fe9949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Bates...\n",
      "Reading Bernstein...\n",
      "Reading Brown...\n",
      "Reading Clark...\n",
      "Reading Demetras2...\n",
      "Reading Gleason...\n",
      "Reading HSLLD...\n",
      "Reading Hall...\n",
      "Reading NewmanRatner...\n",
      "Reading Post...\n",
      "Reading VanHouten...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<pylangacq.chat.Reader at 0x1f4953e37f0>,\n",
       " <pylangacq.chat.Reader at 0x1f493fefbe0>,\n",
       " <pylangacq.chat.Reader at 0x1f4a20af610>,\n",
       " <pylangacq.chat.Reader at 0x1f4a20af460>,\n",
       " <pylangacq.chat.Reader at 0x1f493fef5b0>,\n",
       " <pylangacq.chat.Reader at 0x1f4abcd53d0>,\n",
       " <pylangacq.chat.Reader at 0x1f4dc783970>,\n",
       " <pylangacq.chat.Reader at 0x1f4dc7920a0>,\n",
       " <pylangacq.chat.Reader at 0x1f48ed80f40>,\n",
       " <pylangacq.chat.Reader at 0x1f48edcf2b0>,\n",
       " <pylangacq.chat.Reader at 0x1f48edb4160>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora_to_use = []\n",
    "\n",
    "for corpus_name in search_result:\n",
    "    \n",
    "    # Construct the URL for the corpus from its name:\n",
    "    corpus_url = 'https://childes.talkbank.org/data/Eng-NA/'+corpus_name+'.zip'\n",
    "   \n",
    "    # read the corpus into a Reader object:\n",
    "    print('Reading {}...'.format(corpus_name))\n",
    "    corpus = pylangacq.read_chat(corpus_url)\n",
    "    \n",
    "    # create a list of corpora (as Reader objects) to use in this project\n",
    "    corpora_to_use.append(corpus)\n",
    "\n",
    "corpora_to_use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c32566a-800e-43a8-864f-855a6b09f2f1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2 Basic statistics\n",
    "\n",
    "## 2.1 Token count\n",
    "\n",
    "We can use the methods `.tokens()` and `.utterances` to access the token and utterance information stored in the `Reader` objects. Let's see how many tokens are three in these corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f46d248c-04bb-42c7-a810-8d36815fa793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count in Bates: 56304\n",
      "Token count in Bernstein: 83040\n",
      "Token count in Brown: 880322\n",
      "Token count in Clark: 258699\n",
      "Token count in Demetras2: 99363\n",
      "Token count in Gleason: 317306\n",
      "Token count in HSLLD: 1650889\n",
      "Token count in Hall: 1340000\n",
      "Token count in NewmanRatner: 1049697\n",
      "Token count in Post: 185246\n",
      "Token count in VanHouten: 63884\n",
      "\n",
      "Total token count: 5984750\n"
     ]
    }
   ],
   "source": [
    "all_tokens = []  # To store a list of lists of tokens for all corpora\n",
    "token_sum = 0    # Total token counter\n",
    "\n",
    "for idx, corpus in enumerate(corpora_to_use):\n",
    "    corpus_name = corpora_to_use[idx].headers()[0]['Participants']['CHI']['corpus']\n",
    "    \n",
    "    # Get token info and store them in 'all_tokens':\n",
    "    tokens = corpus.tokens()  # list of Token objects\n",
    "    all_tokens.append(tokens)\n",
    "    \n",
    "    # Print result\n",
    "    print('Token count in {}: {}'.format(corpus_name, len(tokens)))\n",
    "    token_sum = token_sum + len(tokens)\n",
    "\n",
    "# Print result\n",
    "print('\\nTotal token count: {}'.format(token_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0b12ae-0bd9-4a95-a4dc-df10a680b278",
   "metadata": {},
   "source": [
    "\n",
    "## 2.2 Utterance count\n",
    "\n",
    "Similarly, we can access the utterance infomation stored in each `Reader` object. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd72aae1-ce61-45ba-8964-ca244bd93151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>  <tr>\n",
       "    <td>*CHI:</td>\n",
       "    <td style=\"text-align: left\">.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>%gpx:</td>\n",
       "    <td colspan=\"1\" style=\"text-align: left\">looks at chicken</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>%act:</td>\n",
       "    <td colspan=\"1\" style=\"text-align: left\">holds nesting cups</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>%pho:</td>\n",
       "    <td colspan=\"1\" style=\"text-align: left\">wi</td>\n",
       "  </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "Utterance(participant='CHI', tokens=[Token(word='.', pos=None, mor=None, gra=None)], time_marks=None, tiers={'CHI': 'yyy .', '%gpx': 'looks at chicken', '%act': 'holds nesting cups', '%pho': 'wi'})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora_to_use[0].utterances()[1]  # first utterance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542b09fa-850d-4df5-9342-0ade51380155",
   "metadata": {},
   "source": [
    "The example above shows the second utterance and their annotation information in the first corpus ('Bates'), including the words, the speaker and more. To look at child-directed speech (CDS) specifically, we can set the `.utterances()`'s `participants` option to `MOT`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a37dcdd-95ff-4f19-9b9f-9799b10ece16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>  <tr>\n",
       "    <td>*MOT:</td>\n",
       "    <td style=\"text-align: left\">what's</td>\n",
       "    <td style=\"text-align: left\">CLITIC</td>\n",
       "    <td style=\"text-align: left\">that</td>\n",
       "    <td style=\"text-align: left\">?</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>%mor:</td>\n",
       "    <td style=\"text-align: left\">pro:int|what</td>\n",
       "    <td style=\"text-align: left\">cop|be&3S</td>\n",
       "    <td style=\"text-align: left\">pro:dem|that</td>\n",
       "    <td style=\"text-align: left\">?</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>%gra:</td>\n",
       "    <td style=\"text-align: left\">1|2|SUBJ</td>\n",
       "    <td style=\"text-align: left\">2|0|ROOT</td>\n",
       "    <td style=\"text-align: left\">3|2|PRED</td>\n",
       "    <td style=\"text-align: left\">4|2|PUNCT</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>%act:</td>\n",
       "    <td colspan=\"4\" style=\"text-align: left\">holds object out to Amy</td>\n",
       "  </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "Utterance(participant='MOT', tokens=[Token(word=\"what's\", pos='pro:int', mor='what', gra=Gra(dep=1, head=2, rel='SUBJ')), Token(word='CLITIC', pos='cop', mor='be&3S', gra=Gra(dep=2, head=0, rel='ROOT')), Token(word='that', pos='pro:dem', mor='that', gra=Gra(dep=3, head=2, rel='PRED')), Token(word='?', pos='?', mor='', gra=Gra(dep=4, head=2, rel='PUNCT'))], time_marks=None, tiers={'MOT': \"what's that ?\", '%mor': 'pro:int|what~cop|be&3S pro:dem|that ?', '%gra': '1|2|SUBJ 2|0|ROOT 3|2|PRED 4|2|PUNCT', '%act': 'holds object out to Amy'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora_to_use[0].utterances(participants='MOT')[0] # first utterance by mother"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ae966-1f9e-4183-94d4-1f9e60603afb",
   "metadata": {},
   "source": [
    "Next, I will look at how many utterances are there in the data. How many utterances are there in the child speech, and how many in the mother's CDS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "311e9851-9ef7-4461-934a-55fc1777d470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Child utterance count in Bates: 5572\n",
      "Mother utterance count in Bates: 8579\n",
      "Child utterance count in Bernstein: 167\n",
      "Mother utterance count in Bernstein: 11749\n",
      "Child utterance count in Brown: 96952\n",
      "Mother utterance count in Brown: 60252\n",
      "Child utterance count in Clark: 18169\n",
      "Mother utterance count in Clark: 1944\n",
      "Child utterance count in Demetras2: 9411\n",
      "Mother utterance count in Demetras2: 6227\n",
      "Child utterance count in Gleason: 20137\n",
      "Mother utterance count in Gleason: 19545\n",
      "Child utterance count in HSLLD: 112615\n",
      "Mother utterance count in HSLLD: 148301\n",
      "Child utterance count in Hall: 75655\n",
      "Mother utterance count in Hall: 37028\n",
      "Child utterance count in NewmanRatner: 28934\n",
      "Mother utterance count in NewmanRatner: 160889\n",
      "Child utterance count in Post: 8380\n",
      "Mother utterance count in Post: 20189\n",
      "Child utterance count in VanHouten: 5132\n",
      "Mother utterance count in VanHouten: 4770\n",
      "\n",
      "Total child utterance count: 381124\n",
      "\n",
      "Total mother utterance count: 479473\n",
      "\n",
      "Total utterance count: 860597\n",
      "\n",
      "Utterance count percentage: 44.29% by child; 55.71% by mother\n"
     ]
    }
   ],
   "source": [
    "all_utt_chi = []  # To store a list of lists of child utterances for all corpora\n",
    "all_utt_mot = []  # To store a list of lists of mother utterances for all corpora\n",
    "utt_chi_sum = 0   # Total child utterance counter\n",
    "utt_mot_sum = 0   # Total mother utterance counter\n",
    "utt_sum = 0       # Total utterance counter\n",
    "\n",
    "for idx, corpus in enumerate(corpora_to_use):\n",
    "    corpus_name = corpora_to_use[idx].headers()[0]['Participants']['CHI']['corpus']\n",
    "    \n",
    "    # Get utterances and store them in 'all_utt_chi' or 'all_utt_mot':\n",
    "    utt_chi = corpus.utterances(participants='CHI')  # list of Utterance object\n",
    "    utt_mot = corpus.utterances(participants='MOT')  # list of Utterance object\n",
    "    all_utt_chi.append(utt_chi)\n",
    "    all_utt_mot.append(utt_mot)\n",
    "    \n",
    "    # Print results\n",
    "    print('Child utterance count in {}: {}'.format(corpus_name, len(utt_chi)))\n",
    "    print('Mother utterance count in {}: {}'.format(corpus_name, len(utt_mot)))\n",
    "    utt_chi_sum = utt_chi_sum + len(utt_chi)\n",
    "    utt_mot_sum = utt_mot_sum + len(utt_mot)\n",
    "\n",
    "utt_sum = utt_sum + utt_chi_sum + utt_mot_sum\n",
    "utt_chi_pc = round (utt_chi_sum/utt_sum*100, 2)  # Child utterance percentage\n",
    "utt_mot_pc = round (utt_mot_sum/utt_sum*100, 2)  # Mother utterance percentage\n",
    "\n",
    "# Print results\n",
    "print('\\nTotal child utterance count: {}'.format(utt_chi_sum))\n",
    "print('\\nTotal mother utterance count: {}'.format(utt_mot_sum))\n",
    "print('\\nTotal utterance count: {}'.format(utt_sum))\n",
    "print('\\nUtterance count percentage: {}% by child; {}% by mother'\n",
    "      .format(utt_chi_pc, utt_mot_pc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3152a07-b23c-4204-89e6-f20c1b191b97",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3 Data objects for further analysis\n",
    "\n",
    "The above code have created several data objects ready for further analysis, which are the `Reader`, `Token` and the `Utterance` objects. I will pickle these objects so that I don't need create these objects again every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5780e286-db36-4278-bb42-38e0cef73e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data = [corpora_to_use, all_tokens, all_utt_chi, all_utt_mot]\n",
    "\n",
    "f = open('data/childes/selected_corpora.pkl', 'wb')  \n",
    "pickle.dump(data, f, -1)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
