{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0361b10a-3d8d-4fdd-876a-dd275328f930",
   "metadata": {},
   "source": [
    "\n",
    "# Data curation\n",
    "\n",
    "Man Ho Wong | m.wong@pitt.edu | Feb 27th, 2022\n",
    "\n",
    "This notebook search for the datasets needed for this project in the following database:\n",
    "- [CHILDES](https://childes.talkbank.org/)  \n",
    "  *Reference:* MacWhinney, B. (2000). The CHILDES Project: Tools for analyzing talk. Third Edition. Mahwah, NJ: Lawrence Erlbaum Associates.\n",
    "\n",
    "I may not need datasets from [Wordbank](http://wordbank.stanford.edu/) as I found that CHILDES probably has all the data I need.\n",
    "\n",
    "I will also explore the datasets on the way to get a sense of the contents and the structures of the datasets (such as participant information, annotations, data format, etc.), as well as some basic statistics about the datasets. After that, I will identify the information I need in the datasets and compile the data for data processing later.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2624715b-d6a5-41f6-b7bc-adf57127d855",
   "metadata": {},
   "source": [
    "\n",
    "# 1 Searching for suitable corpora in CHILDES\n",
    "\n",
    "CHILDES is a multilingual database containing corpora with transcriptions, audio recordings and/or video recordings of child speech and child-directed speech (CDS) at different developmental stages. Each corpus has a separate directory for each participant, and each directory contains the recording transcripts stored in CHAT formats. ([Example](https://childes.talkbank.org/access/Eng-NA/Brown.html))\n",
    "\n",
    "For this project, I will need to collect the transcipts for both the child speech and the associated CDS. Additionally, I will need the participant information (i.e. child age, sex and socioeconomic status (SES), mother's education) and some basic annotations of the words (i.e. morphemes and lexical categories). Participant information can be found in the header of each CHAT file as the metadata of the file. Annotation information can be found as dependent tiers embedded in the transcription.\n",
    "\n",
    "I will first search for the datasets meeting the needs of the project. Here are the basic search criteria:\n",
    "- Language: North American English (Note: I may need other languages later for comparison)\n",
    "- Participants: contains only child and mother (i.e. other people such as investigators were not involved)\n",
    "- Child information: contains child age, sex and socioeconomic status (SES)\n",
    "- Mother information: contains socioeconomic status (SES), education\n",
    "- Annotation: contains morpheme and/or grammatical tiers\n",
    "\n",
    "Let's take a quick look at a sample CHAT file first to see how the data is organized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98686cc-77f7-4f43-91dd-4575e7261f98",
   "metadata": {},
   "source": [
    "\n",
    "## 1.1 Reading CHAT file\n",
    "\n",
    "The `PyLangAcq` package allows users to read CHAT files directly from a zip file. You can download and install it with the following code:  \n",
    "`$ pip install --upgrade pylangacq`\n",
    "\n",
    "For documentation, you can visit their [website](https://pylangacq.org/).\n",
    "\n",
    "I will use the Brown Corpus of CHILDES as an example below. The corpus has been downloaded from [here](https://childes.talkbank.org/data/Eng-NA/Brown.zip) and stored under `data_samples/childes/Brown.zip`. There are three folders in this corpus, each folder contains a dataset (a collection of CHAT files) for each child:\n",
    "\n",
    "```\n",
    "Brown.zip/  \n",
    "    |--Adam/  \n",
    "    |--Eve/  \n",
    "    |--Sarah/\n",
    "```\n",
    "\n",
    "I will use the `read_chat()` function of `PyLangAcq` to read all the CHAT files in the dataset `Adam`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce9e997b-a6eb-4ede-b8d2-eba6cc55bab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pylangacq.chat.Reader'>\n",
      "Number of CHAT files: 55\n"
     ]
    }
   ],
   "source": [
    "import pylangacq\n",
    "\n",
    "# Read CHAT files in the dataset 'Adam' in 'Brown.zip':\n",
    "path = 'data_samples/childes/Brown.zip'\n",
    "adam = pylangacq.read_chat(path, 'Adam')\n",
    "\n",
    "print(type(adam))\n",
    "print('Number of CHAT files:', adam.n_files())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eda0575c-87d5-458f-915e-47a7737f4bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ages (year, month, day): [(2, 3, 4), (2, 3, 18), (2, 4, 3), (2, 4, 15), (2, 4, 30), (2, 5, 12), (2, 6, 3), (2, 6, 17), (2, 7, 1), (2, 7, 14), (2, 8, 1), (2, 8, 16), (2, 9, 4), (2, 9, 18), (2, 10, 2), (2, 10, 16), (2, 10, 30), (2, 11, 13), (2, 11, 28), (3, 0, 11), (3, 0, 25), (3, 1, 9), (3, 1, 26), (3, 2, 9), (3, 2, 21), (3, 3, 4), (3, 3, 18), (3, 4, 1), (3, 4, 18), (3, 5, 1), (3, 5, 15), (3, 5, 29), (3, 6, 9), (3, 7, 7), (3, 8, 1), (3, 8, 14), (3, 8, 26), (3, 9, 16), (3, 10, 15), (3, 11, 1), (3, 11, 14), (4, 0, 14), (4, 1, 15), (4, 2, 17), (4, 3, 9), (4, 4, 1), (4, 4, 13), (4, 5, 11), (4, 6, 24), (4, 7, 1), (4, 7, 29), (4, 9, 2), (4, 10, 2), (4, 10, 23), (5, 2, 12)]\n"
     ]
    }
   ],
   "source": [
    "# Ages when recordings were made\n",
    "print('Ages (year, month, day):', adam.ages())  # output: a list of tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4f47d0-163c-498c-81a2-7828c9270d7d",
   "metadata": {},
   "source": [
    "As shown above, `read_chat()` read the CHAT files and creates a `Reader` object. This is a `dataclass` storing data and metadata across all the CHAT files in `Adam`. You can access the data stored in the `Reader` by calling the appropriate methods, such as `.n_files()` for number of CHAT files in the dataset. For example, `Adam` has 55 CHAT files. We can also get the ages when recordings were made by calling `.ages()`. Let's see what other information we can get from the `Reader` object in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb97d197-845e-46e4-a494-cd3765572cb0",
   "metadata": {},
   "source": [
    "## 1.2 Accessing metadata stored in a CHAT file\n",
    "\n",
    "Metadata such as age range, date of recording, participants, etc. are stored in the header of each CHAT file. We can access these information by calling the `.header()` method. Here is the header for the first CHAT file in `adam`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caad807e-c699-4e50-9dfc-f9663e6c8f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UTF8': '',\n",
       " 'PID': '11312/c-00015632-1',\n",
       " 'Languages': ['eng'],\n",
       " 'Participants': {'CHI': {'name': 'Adam',\n",
       "   'language': 'eng',\n",
       "   'corpus': 'Brown',\n",
       "   'age': '2;03.04',\n",
       "   'sex': 'male',\n",
       "   'group': 'TD',\n",
       "   'ses': 'MC',\n",
       "   'role': 'Target_Child',\n",
       "   'education': '',\n",
       "   'custom': ''},\n",
       "  'MOT': {'name': 'Mother',\n",
       "   'language': 'eng',\n",
       "   'corpus': 'Brown',\n",
       "   'age': '',\n",
       "   'sex': 'female',\n",
       "   'group': '',\n",
       "   'ses': '',\n",
       "   'role': 'Mother',\n",
       "   'education': '',\n",
       "   'custom': ''},\n",
       "  'URS': {'name': 'Ursula_Bellugi',\n",
       "   'language': 'eng',\n",
       "   'corpus': 'Brown',\n",
       "   'age': '',\n",
       "   'sex': '',\n",
       "   'group': '',\n",
       "   'ses': '',\n",
       "   'role': 'Investigator',\n",
       "   'education': '',\n",
       "   'custom': ''},\n",
       "  'RIC': {'name': 'Richard_Cromer',\n",
       "   'language': 'eng',\n",
       "   'corpus': 'Brown',\n",
       "   'age': '',\n",
       "   'sex': '',\n",
       "   'group': '',\n",
       "   'ses': '',\n",
       "   'role': 'Investigator',\n",
       "   'education': '',\n",
       "   'custom': ''},\n",
       "  'COL': {'name': 'Colin_Fraser',\n",
       "   'language': 'eng',\n",
       "   'corpus': 'Brown',\n",
       "   'age': '',\n",
       "   'sex': '',\n",
       "   'group': '',\n",
       "   'ses': '',\n",
       "   'role': 'Investigator',\n",
       "   'education': '',\n",
       "   'custom': ''}},\n",
       " 'Date': {datetime.date(1962, 10, 8), datetime.date(1962, 10, 9)},\n",
       " 'Comment': 'Birth of CHI is 4-JUL-1960',\n",
       " 'Time Duration': '15:00-16:00',\n",
       " 'Types': 'long, toyplay, TD'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam.headers()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29beeb4-d948-4c36-b2f3-eab0a9095ffb",
   "metadata": {},
   "source": [
    "The output above, is a multilevel `dictionary`. TO retrieve a specific piece of information we need, we can use the `dictionary` keys as usual.  \n",
    "Let's check if 'Adam' is a male as its biblical name suggests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4850e46-5e97-4e20-8a06-6aea94ef4090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam.headers()[0]['Participants']['CHI']['sex']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad9708f-391c-4edf-a245-25e4c2d06095",
   "metadata": {},
   "source": [
    "\n",
    "## 1.3 Accessing annotations\n",
    "\n",
    "Next, I will check what kinds of annotation information are stored in each CHAT file. I will use the `.tokens()` method to access the tokens with annotation information. This method creates a `list` of `Token` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d0375c-caed-42a4-9afc-d121b3f79e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(word='play', pos='n', mor='play', gra=Gra(dep=1, head=2, rel='MOD')),\n",
       " Token(word='checkers', pos='n', mor='checker-PL', gra=Gra(dep=2, head=0, rel='INCROOT')),\n",
       " Token(word='.', pos='.', mor='', gra=Gra(dep=3, head=2, rel='PUNCT')),\n",
       " Token(word='big', pos='adj', mor='big', gra=Gra(dep=1, head=2, rel='MOD')),\n",
       " Token(word='drum', pos='n', mor='drum', gra=Gra(dep=2, head=0, rel='INCROOT'))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = adam.tokens()\n",
    "tokens[:5]  # first five tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f07287-85f1-45ef-8234-a647f9a4fd24",
   "metadata": {},
   "source": [
    "Each `Token` is a `dataclass` with attributes (e.g. `word`,`pos`, etc.) as shown in the above example.  \n",
    "Annotations for each word are stored as the `Token`'s attributes (i.e. attributes other than `word`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2bf4834-9464-4b45-9c7e-3d8074646518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second token in 'Adam':\n",
      "Word: checkers\n",
      "Morpheme: checker-PL\n",
      "Part of speech: n\n"
     ]
    }
   ],
   "source": [
    "print(\"Second token in 'Adam':\")\n",
    "print('Word: {}\\nMorpheme: {}\\nPart of speech: {}'.format(\n",
    "    tokens[1].word, tokens[1].mor, tokens[1].pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a51c99-28b0-4d98-97ec-8a61817ab671",
   "metadata": {},
   "source": [
    "\n",
    "## 1.4 Searching for suitable corpora\n",
    "\n",
    "Now that we know what kinds of information are stored in each dataset and how we can access them, we can start searching for the corpora for the project according to the criteria set previously.\n",
    "\n",
    "There are dozens of English corpora in CHILDES. We don't need to download them all at once just to look for the corpora we need. `PyLangAcq` allows user to read a corpus directly with the corpus's URL. We can read the corpora one by one, and keep only the ones we need. To get the URLs for all the North American English corpora, one can use some web scraping tools to get all the links from the database's website and look for the corpus URLs from there. However, a much faster way is to take advantage of the TalkBank's [browsable database](https://sla.talkbank.org/TBB/childes):  \n",
    "1. navigate to CHILDES's North American English datasets (Eng-NA)\n",
    "2. copy the list of corpora directly to a spreadsheet program and save it as a `csv` file (example: `data_samples/childes/eng_NA_corpus_list.csv`)\n",
    "3. construct the URL for each corpus simply from the name of the corpus we get from step 2 (see below)\n",
    "\n",
    "CHILDES has a very well organized structure. Each corpus has the same URL format as follow:  \n",
    "`https://childes.talkbank.org/data/LANGUAGE/NAME_OF_CORPUS.zip`  \n",
    "For example, the URL for the Brown Corpus is: https://childes.talkbank.org/data/Eng-NA/Brown.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7132000-53ab-4b6b-b187-4f647de52636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 47 NA English corpora in CHILDES.\n",
      "Here are the first 10 corpora:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        Bates\n",
       "1    Bernstein\n",
       "2        Bliss\n",
       "3        Bloom\n",
       "4     Bohannon\n",
       "5    Braunwald\n",
       "6        Brent\n",
       "7        Brown\n",
       "8        Clark\n",
       "9    Demetras1\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the list of corpora into a Pandas Series:\n",
    "corpus_list = pd.read_csv('data/childes/eng_NA_corpus_list.csv', \n",
    "                          header=None, index_col=False, squeeze = True)\n",
    "\n",
    "print('There are {} NA English corpora in CHILDES.'.format(len(corpus_list)))\n",
    "print('Here are the first 10 corpora:')\n",
    "corpus_list.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d611aa-aa63-4fac-8306-a8e25464cd0a",
   "metadata": {},
   "source": [
    "There are 47 corpora in CHILDES that we can potentially use! Let's try to look for the corpora matching some of the criteria set previously, e.g. corpora containing information about child's/ mother's SES and educational background. As shown previously, we can use `.headers` to access these information in the CHAT files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a43cb57-1335-432f-9ba8-bbb473cf9de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Bates...\n",
      "Bates matches the criteria!\n",
      "\n",
      "Searching Bernstein...\n",
      "Bernstein matches the criteria!\n",
      "\n",
      "Searching Bliss...\n",
      "Searching Bloom...\n",
      "Searching Bohannon...\n",
      "Searching Braunwald...\n",
      "Searching Brent...\n",
      "Searching Brown...\n",
      "Brown matches the criteria!\n",
      "\n",
      "Searching Clark...\n",
      "Clark matches the criteria!\n",
      "\n",
      "Searching Demetras1...\n",
      "Searching Demetras2...\n",
      "Demetras2 matches the criteria!\n",
      "\n",
      "Searching Evans...\n",
      "Searching Feldman...\n",
      "Searching Garvey...\n",
      "Searching Gathercole...\n",
      "Searching Gelman...\n",
      "Searching Gleason...\n",
      "Gleason matches the criteria!\n",
      "\n",
      "Searching Gopnik...\n",
      "Searching HSLLD...\n",
      "HSLLD matches the criteria!\n",
      "\n",
      "Searching Haggerty...\n",
      "Searching Hall...\n",
      "Hall matches the criteria!\n",
      "\n",
      "Searching Hicks...\n",
      "Searching Higginson...\n",
      "Searching Kuczaj...\n",
      "Searching MacWhinney...\n",
      "Searching McCune...\n",
      "Searching McMillan...\n",
      "Searching Morisset...\n",
      "Searching Nelson...\n",
      "Searching NewEngland...\n",
      "Searching NewmanRatner...\n",
      "NewmanRatner matches the criteria!\n",
      "\n",
      "Searching Peters...\n",
      "Searching PetersonMcCabe...\n",
      "Searching Post...\n",
      "Post matches the criteria!\n",
      "\n",
      "Searching Rollins...\n",
      "Searching Sachs...\n",
      "Searching Sawyer...\n",
      "Searching Snow...\n",
      "Searching Soderstrom...\n",
      "Searching Sprott...\n",
      "Searching Suppes...\n",
      "Searching Tardif...\n",
      "Searching Valian...\n",
      "Searching VanHouten...\n",
      "VanHouten matches the criteria!\n",
      "\n",
      "Searching VanKleeck...\n",
      "Searching Warren...\n",
      "Searching Weist...\n",
      "Search completed!\n"
     ]
    }
   ],
   "source": [
    "search_result = []  # To store a list of corpora matching the criteria\n",
    "\n",
    "# Search each corpus in the list:\n",
    "for corpus_name in corpus_list:\n",
    "    \n",
    "    # Construct the URL for the corpus from its name:\n",
    "    corpus_url = 'https://childes.talkbank.org/data/Eng-NA/'+corpus_name+'.zip'\n",
    "    \n",
    "    # read the corpus into a Reader object:\n",
    "    print('Searching {}...'.format(corpus_name))\n",
    "    corpus = pylangacq.read_chat(corpus_url)\n",
    "    \n",
    "    # Inspect each CHAT file in the corpus:\n",
    "    found = False    \n",
    "    for f in range(corpus.n_files()):\n",
    "        \n",
    "        # Search criteria:        \n",
    "        if (\n",
    "            # The child should not be older than 72 months (will refine this later).\n",
    "             ((corpus.ages()[f][0]*12 + corpus.ages()[f][1]) <= 72) \n",
    "            \n",
    "            and\n",
    "            \n",
    "            # Dataset must contain child ('CHI') and mother ('MOT') as participants\n",
    "            # (as we are looking at child-directed speech).\n",
    "            (('CHI' in corpus.headers()[f]['Participants']) and \n",
    "            ('MOT' in corpus.headers()[f]['Participants']))\n",
    "            \n",
    "            and\n",
    "            \n",
    "            # Dataset should include child's or mother's SES/education.\n",
    "            ((corpus.headers()[f]['Participants']['MOT']['ses'] != '') or\n",
    "             (corpus.headers()[f]['Participants']['CHI']['ses'] != '') or\n",
    "             (corpus.headers()[f]['Participants']['MOT']['education'] != ''))\n",
    "        ):\n",
    "            \n",
    "            print('{} matches the criteria!\\n'.format(corpus_name))\n",
    "            search_result.append(corpus_name) \n",
    "            found = True\n",
    "            \n",
    "        # Break the for loop if criteria are matched and move on to another\n",
    "        # corpus. (If one CHAT file contains the needed information, I will keep\n",
    "        # the whole corpus for data processing later and remove individual CHAT\n",
    "        # files missing the needed information.)\n",
    "        if found == True:\n",
    "            break\n",
    "\n",
    "print('Search completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ab9831-c39c-42eb-a9eb-5ce182450676",
   "metadata": {},
   "source": [
    "Let's see which corpora contain the data we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a281639-489c-4578-86dc-6a779ac693e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11 corpora matching the criteria:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Bates',\n",
       " 'Bernstein',\n",
       " 'Brown',\n",
       " 'Clark',\n",
       " 'Demetras2',\n",
       " 'Gleason',\n",
       " 'HSLLD',\n",
       " 'Hall',\n",
       " 'NewmanRatner',\n",
       " 'Post',\n",
       " 'VanHouten']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\n{} corpora matching the criteria:'.format(len(search_result)))\n",
    "search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90501fb-2470-4586-9544-7c5efcbdff7e",
   "metadata": {},
   "source": [
    "Nice! We have narrowed down the number of corpora we need to process from 47 to 11.  \n",
    "Next, I will download the zip files for the corpora to my local drive and extract them to the folder `data/childes/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1802310c-4a91-455b-8873-4b9ad0caa838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting Bates...\n",
      "Downloading and extracting Bernstein...\n",
      "Downloading and extracting Brown...\n",
      "Downloading and extracting Clark...\n",
      "Downloading and extracting Demetras2...\n",
      "Downloading and extracting Gleason...\n",
      "Downloading and extracting HSLLD...\n",
      "Downloading and extracting Hall...\n",
      "Downloading and extracting NewmanRatner...\n",
      "Downloading and extracting Post...\n",
      "Downloading and extracting VanHouten...\n",
      "Done! Zip files were stored in data/childes/corpus_zip/ \n",
      "and extracted to data/childes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Where zip files will be stored locally\n",
    "target_dir = 'data/childes/corpus_zip/'\n",
    "\n",
    "# Create a folder to store the zip files\n",
    "os.mkdir(target_dir)\n",
    "\n",
    "for corpus_name in search_result:\n",
    "    \n",
    "    print('Downloading and extracting {}...'.format(corpus_name))\n",
    "    \n",
    "    # Corpus's download URL:\n",
    "    corpus_url = 'https://childes.talkbank.org/data/Eng-NA/'+corpus_name+'.zip'\n",
    "    \n",
    "    # Path in local drive:\n",
    "    zip_path = target_dir + corpus_name + '.zip'\n",
    "    \n",
    "    # Download corpus from URL\n",
    "    urllib.request.urlretrieve(corpus_url, zip_path)\n",
    "    \n",
    "    # Extract zip file\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        z.extractall('data/childes')\n",
    "        \n",
    "print('Done! Zip files were stored in {} \\nand extracted to {}.'\n",
    "      .format(target_dir, 'data/childes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d14ea2-e6d3-43b2-b0f6-59a6a5639444",
   "metadata": {},
   "source": [
    "We have downloaded all the corpora containing any number of CHAT files matching our search criteria. Since not all the files in the same corpus match the search criteria, I will not use all the files for the project. I will create a `DataFrame` as an index pointing to the files that meet the search criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "56142f20-ba10-4a52-b41c-2cd2404300c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Bates...\n",
      "Reading Bernstein...\n",
      "Reading Brown...\n",
      "Reading Clark...\n",
      "Reading Demetras2...\n",
      "Reading Gleason...\n",
      "Reading HSLLD...\n",
      "Reading Hall...\n",
      "Reading NewmanRatner...\n",
      "Reading Post...\n",
      "Reading VanHouten...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>corpus</th>\n",
       "      <th>name</th>\n",
       "      <th>age_d</th>\n",
       "      <th>age_m</th>\n",
       "      <th>sex</th>\n",
       "      <th>group</th>\n",
       "      <th>ses</th>\n",
       "      <th>mot_edu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/childes/Bates/Free20/amy.cha</td>\n",
       "      <td>Bates</td>\n",
       "      <td>Target_Child</td>\n",
       "      <td>600</td>\n",
       "      <td>20.0</td>\n",
       "      <td>female</td>\n",
       "      <td>TD</td>\n",
       "      <td>MC</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/childes/Bates/Free20/betty.cha</td>\n",
       "      <td>Bates</td>\n",
       "      <td>Betty</td>\n",
       "      <td>600</td>\n",
       "      <td>20.0</td>\n",
       "      <td>female</td>\n",
       "      <td>TD</td>\n",
       "      <td>MC</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/childes/Bates/Free20/chuck.cha</td>\n",
       "      <td>Bates</td>\n",
       "      <td>Chuck</td>\n",
       "      <td>600</td>\n",
       "      <td>20.0</td>\n",
       "      <td>male</td>\n",
       "      <td>TD</td>\n",
       "      <td>MC</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/childes/Bates/Free20/doug.cha</td>\n",
       "      <td>Bates</td>\n",
       "      <td>Doug</td>\n",
       "      <td>600</td>\n",
       "      <td>20.0</td>\n",
       "      <td>male</td>\n",
       "      <td>TD</td>\n",
       "      <td>MC</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/childes/Bates/Free20/ed.cha</td>\n",
       "      <td>Bates</td>\n",
       "      <td>Ed</td>\n",
       "      <td>600</td>\n",
       "      <td>20.0</td>\n",
       "      <td>male</td>\n",
       "      <td>TD</td>\n",
       "      <td>MC</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>data/childes/VanHouten/Twos/teaching/parkt.cha</td>\n",
       "      <td>VanHouten</td>\n",
       "      <td>Matthew</td>\n",
       "      <td>840</td>\n",
       "      <td>28.0</td>\n",
       "      <td>male</td>\n",
       "      <td>MOT_Older</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>data/childes/VanHouten/Twos/teaching/pricet.cha</td>\n",
       "      <td>VanHouten</td>\n",
       "      <td>Peter</td>\n",
       "      <td>840</td>\n",
       "      <td>28.0</td>\n",
       "      <td>male</td>\n",
       "      <td>MOT_adolescent</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>data/childes/VanHouten/Twos/teaching/raidt.cha</td>\n",
       "      <td>VanHouten</td>\n",
       "      <td>Tommy</td>\n",
       "      <td>840</td>\n",
       "      <td>28.0</td>\n",
       "      <td>male</td>\n",
       "      <td>MOT_older</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>data/childes/VanHouten/Twos/teaching/riott.cha</td>\n",
       "      <td>VanHouten</td>\n",
       "      <td>Robert</td>\n",
       "      <td>840</td>\n",
       "      <td>28.0</td>\n",
       "      <td>male</td>\n",
       "      <td>MOT_Adolescent</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>data/childes/VanHouten/Twos/teaching/royalt.cha</td>\n",
       "      <td>VanHouten</td>\n",
       "      <td>Sarah</td>\n",
       "      <td>840</td>\n",
       "      <td>28.0</td>\n",
       "      <td>female</td>\n",
       "      <td>MOT_Adolescent</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2277 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            file_path     corpus  \\\n",
       "0                   data/childes/Bates/Free20/amy.cha      Bates   \n",
       "1                 data/childes/Bates/Free20/betty.cha      Bates   \n",
       "2                 data/childes/Bates/Free20/chuck.cha      Bates   \n",
       "3                  data/childes/Bates/Free20/doug.cha      Bates   \n",
       "4                    data/childes/Bates/Free20/ed.cha      Bates   \n",
       "...                                               ...        ...   \n",
       "2272   data/childes/VanHouten/Twos/teaching/parkt.cha  VanHouten   \n",
       "2273  data/childes/VanHouten/Twos/teaching/pricet.cha  VanHouten   \n",
       "2274   data/childes/VanHouten/Twos/teaching/raidt.cha  VanHouten   \n",
       "2275   data/childes/VanHouten/Twos/teaching/riott.cha  VanHouten   \n",
       "2276  data/childes/VanHouten/Twos/teaching/royalt.cha  VanHouten   \n",
       "\n",
       "              name  age_d  age_m     sex           group ses mot_edu  \n",
       "0     Target_Child    600   20.0  female              TD  MC          \n",
       "1            Betty    600   20.0  female              TD  MC          \n",
       "2            Chuck    600   20.0    male              TD  MC          \n",
       "3             Doug    600   20.0    male              TD  MC          \n",
       "4               Ed    600   20.0    male              TD  MC          \n",
       "...            ...    ...    ...     ...             ...  ..     ...  \n",
       "2272       Matthew    840   28.0    male       MOT_Older              \n",
       "2273         Peter    840   28.0    male  MOT_adolescent              \n",
       "2274         Tommy    840   28.0    male       MOT_older              \n",
       "2275        Robert    840   28.0    male  MOT_Adolescent              \n",
       "2276         Sarah    840   28.0  female  MOT_Adolescent              \n",
       "\n",
       "[2277 rows x 9 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_list = []\n",
    "corpus_name_list = []\n",
    "chi_name_list = []\n",
    "chi_age_d_list = []\n",
    "chi_age_m_list = []\n",
    "chi_sex_list = []\n",
    "chi_group_list = []\n",
    "chi_ses_list = []\n",
    "mot_edu_list = []\n",
    "\n",
    "for corpus_name in search_result:\n",
    "    print('Reading {}...'.format(corpus_name))\n",
    "    corpus = pylangacq.Reader.from_dir('Data/childes/'+corpus_name)\n",
    "    \n",
    "    for f in range(corpus.n_files()):\n",
    "        \n",
    "        if (\n",
    "            # Participants must include both the child and the mother.\n",
    "            (('CHI' in corpus.headers()[f]['Participants']) and \n",
    "            ('MOT' in corpus.headers()[f]['Participants'])) \n",
    "            \n",
    "            and\n",
    "            \n",
    "            # Dataset should include child's or mother's SES/education.\n",
    "            (( 'ses' in corpus.headers()[f]['Participants']['MOT']) or\n",
    "             (corpus.headers()[f]['Participants']['CHI']['ses'] != '') or\n",
    "             (corpus.headers()[f]['Participants']['MOT']['education'] != ''))\n",
    "        ):\n",
    "            \n",
    "            age_raw     = corpus.ages()[f]  # age in raw format\n",
    "            chi_age_d   = (age_raw[0]*12 + age_raw[1])*30 + age_raw[2]\n",
    "            chi_age_m   = age_raw[0]*12 + age_raw[1] + round(age_raw[2]/30,1)\n",
    "\n",
    "            # Include only children <= 72 months:\n",
    "            if chi_age_m <= 72: \n",
    "            \n",
    "                file_path   = corpus.file_paths()[f].replace('\\\\','/')\n",
    "                file_path   = file_path[0].lower() + file_path[1:]\n",
    "                \n",
    "                corpus_name = corpus.headers()[f]['Participants']['CHI']['corpus']\n",
    "                chi_name    = corpus.headers()[f]['Participants']['CHI']['name']\n",
    "                chi_sex     = corpus.headers()[f]['Participants']['CHI']['sex']\n",
    "                chi_group   = corpus.headers()[f]['Participants']['CHI']['group']\n",
    "                chi_ses     = corpus.headers()[f]['Participants']['CHI']['ses']\n",
    "                mot_edu     = corpus.headers()[f]['Participants']['MOT']['education']\n",
    "\n",
    "                file_path_list.append(file_path) \n",
    "                corpus_name_list.append(corpus_name)\n",
    "                chi_name_list.append(chi_name)\n",
    "                chi_age_d_list.append(chi_age_d)\n",
    "                chi_age_m_list.append(chi_age_m)\n",
    "                chi_sex_list.append(chi_sex)\n",
    "                chi_group_list.append(chi_group)\n",
    "                chi_ses_list.append(chi_ses)\n",
    "                mot_edu_list.append(mot_edu)\n",
    "\n",
    "data_idx = pd.DataFrame({'file_path':file_path_list,\n",
    "                         'corpus':corpus_name_list,\n",
    "                         'name':chi_name_list,\n",
    "                         'age_d':chi_age_d_list,\n",
    "                         'age_m':chi_age_m_list,\n",
    "                         'sex':chi_sex_list,\n",
    "                         'group':chi_group_list,\n",
    "                         'ses':chi_ses_list,\n",
    "                         'mot_edu':mot_edu_list\n",
    "                        })\n",
    "\n",
    "print('Done!')\n",
    "data_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c32566a-800e-43a8-864f-855a6b09f2f1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2 Basic statistics\n",
    "\n",
    "## 2.1 Token count\n",
    "\n",
    "We can use the methods `.tokens()` and `.utterances` to access the token and utterance information stored in the `Reader` objects. Let's see how many tokens are three in these corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f46d248c-04bb-42c7-a810-8d36815fa793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count in Bates: 56304\n",
      "Token count in Bernstein: 73531\n",
      "Token count in Brown: 880322\n",
      "Token count in Clark: 201560\n",
      "Token count in Demetras2: 51744\n",
      "Token count in Gleason: 205923\n",
      "Token count in HSLLD: 1166539\n",
      "Token count in Hall: 1268553\n",
      "Token count in NewmanRatner: 1046875\n",
      "Token count in Post: 185246\n",
      "Token count in VanHouten: 40200\n",
      "\n",
      "Total token count: 5176797\n"
     ]
    }
   ],
   "source": [
    "all_tokens = []  # To store a list of lists of tokens for all corpora\n",
    "token_sum = 0    # Total token counter\n",
    "\n",
    "for corpus in search_result:\n",
    "    files = data_idx[data_idx.corpus==corpus].file_path\n",
    "    reader = pylangacq.Reader.from_files(files)\n",
    "    # Get token info and store them in 'all_tokens':\n",
    "    tokens = reader.tokens()  # list of Token objects\n",
    "    all_tokens.append(tokens)\n",
    "    \n",
    "    # Print result\n",
    "    print('Token count in {}: {}'.format(corpus, len(tokens)))\n",
    "    token_sum = token_sum + len(tokens)\n",
    "\n",
    "# Print result\n",
    "print('\\nTotal token count: {}'.format(token_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0b12ae-0bd9-4a95-a4dc-df10a680b278",
   "metadata": {},
   "source": [
    "\n",
    "## 2.2 Utterance count\n",
    "\n",
    "Similarly, we can access the utterance infomation stored in each `Reader` object. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cd72aae1-ce61-45ba-8964-ca244bd93151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>  <tr>\n",
       "    <td>*CHI:</td>\n",
       "    <td style=\"text-align: left\">yeah</td>\n",
       "    <td style=\"text-align: left\">.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>%mor:</td>\n",
       "    <td style=\"text-align: left\">co|yeah</td>\n",
       "    <td style=\"text-align: left\">.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>%gra:</td>\n",
       "    <td style=\"text-align: left\">1|0|INCROOT</td>\n",
       "    <td style=\"text-align: left\">2|1|PUNCT</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>%act:</td>\n",
       "    <td colspan=\"2\" style=\"text-align: left\">takes chicken</td>\n",
       "  </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "Utterance(participant='CHI', tokens=[Token(word='yeah', pos='co', mor='yeah', gra=Gra(dep=1, head=0, rel='INCROOT')), Token(word='.', pos='.', mor='', gra=Gra(dep=2, head=1, rel='PUNCT'))], time_marks=None, tiers={'CHI': 'yeah .', '%mor': 'co|yeah .', '%gra': '1|0|INCROOT 2|1|PUNCT', '%act': 'takes chicken'})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = data_idx.file_path[0]  # first file in 'data_idx'\n",
    "reader = pylangacq.Reader.from_files([file])\n",
    "reader.utterances()[3]  # 4th utterance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542b09fa-850d-4df5-9342-0ade51380155",
   "metadata": {},
   "source": [
    "The example above shows the second utterance and their annotation information in the first corpus ('Bates'), including the words, the speaker and more. To look at child-directed speech (CDS) specifically, we can set the `.utterances()`'s `participants` option to `MOT`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4a37dcdd-95ff-4f19-9b9f-9799b10ece16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>  <tr>\n",
       "    <td>*MOT:</td>\n",
       "    <td style=\"text-align: left\">what's</td>\n",
       "    <td style=\"text-align: left\">CLITIC</td>\n",
       "    <td style=\"text-align: left\">that</td>\n",
       "    <td style=\"text-align: left\">?</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>%mor:</td>\n",
       "    <td style=\"text-align: left\">pro:int|what</td>\n",
       "    <td style=\"text-align: left\">cop|be&3S</td>\n",
       "    <td style=\"text-align: left\">pro:dem|that</td>\n",
       "    <td style=\"text-align: left\">?</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>%gra:</td>\n",
       "    <td style=\"text-align: left\">1|2|SUBJ</td>\n",
       "    <td style=\"text-align: left\">2|0|ROOT</td>\n",
       "    <td style=\"text-align: left\">3|2|PRED</td>\n",
       "    <td style=\"text-align: left\">4|2|PUNCT</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>%act:</td>\n",
       "    <td colspan=\"4\" style=\"text-align: left\">holds object out to Amy</td>\n",
       "  </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "Utterance(participant='MOT', tokens=[Token(word=\"what's\", pos='pro:int', mor='what', gra=Gra(dep=1, head=2, rel='SUBJ')), Token(word='CLITIC', pos='cop', mor='be&3S', gra=Gra(dep=2, head=0, rel='ROOT')), Token(word='that', pos='pro:dem', mor='that', gra=Gra(dep=3, head=2, rel='PRED')), Token(word='?', pos='?', mor='', gra=Gra(dep=4, head=2, rel='PUNCT'))], time_marks=None, tiers={'MOT': \"what's that ?\", '%mor': 'pro:int|what~cop|be&3S pro:dem|that ?', '%gra': '1|2|SUBJ 2|0|ROOT 3|2|PRED 4|2|PUNCT', '%act': 'holds object out to Amy'})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.utterances(participants='MOT')[0] # first utterance by mother"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ae966-1f9e-4183-94d4-1f9e60603afb",
   "metadata": {},
   "source": [
    "Next, I will look at how many utterances are there in the data. How many utterances are there in the child speech, and how many in the mother's CDS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "311e9851-9ef7-4461-934a-55fc1777d470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Child utterance count in Bates: 5572\n",
      "Mother utterance count in Bates: 8579\n",
      "Child utterance count in Bernstein: 167\n",
      "Mother utterance count in Bernstein: 11160\n",
      "Child utterance count in Brown: 96952\n",
      "Mother utterance count in Brown: 60252\n",
      "Child utterance count in Clark: 14163\n",
      "Mother utterance count in Clark: 1944\n",
      "Child utterance count in Demetras2: 4431\n",
      "Mother utterance count in Demetras2: 6227\n",
      "Child utterance count in Gleason: 12435\n",
      "Mother utterance count in Gleason: 19545\n",
      "Child utterance count in HSLLD: 76816\n",
      "Mother utterance count in HSLLD: 110117\n",
      "Child utterance count in Hall: 71168\n",
      "Mother utterance count in Hall: 37028\n",
      "Child utterance count in NewmanRatner: 28934\n",
      "Mother utterance count in NewmanRatner: 160717\n",
      "Child utterance count in Post: 8380\n",
      "Mother utterance count in Post: 20189\n",
      "Child utterance count in VanHouten: 3006\n",
      "Mother utterance count in VanHouten: 4770\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "Total child utterance count: 322024\n",
      "\n",
      "Total mother utterance count: 440528\n",
      "\n",
      "Total utterance count: 762552\n",
      "\n",
      "Utterance count percentage: 42.23% by child; 57.77% by mother\n"
     ]
    }
   ],
   "source": [
    "all_utt_chi = []  # To store a list of lists of child utterances for all corpora\n",
    "all_utt_mot = []  # To store a list of lists of mother utterances for all corpora\n",
    "utt_chi_sum = 0   # Total child utterance counter\n",
    "utt_mot_sum = 0   # Total mother utterance counter\n",
    "utt_sum = 0       # Total utterance counter\n",
    "\n",
    "for corpus in search_result:\n",
    "    files = data_idx[data_idx.corpus==corpus].file_path\n",
    "    reader = pylangacq.Reader.from_files(files)\n",
    "    # Get utterances and store them in 'all_utt_chi' or 'all_utt_mot':\n",
    "    utt_chi = reader.utterances(participants='CHI')  # list of Utterance object\n",
    "    utt_mot = reader.utterances(participants='MOT')  # list of Utterance object\n",
    "    all_utt_chi.append(utt_chi)\n",
    "    all_utt_mot.append(utt_mot)\n",
    "    \n",
    "    # Print results\n",
    "    print('Child utterance count in {}: {}'.format(corpus, len(utt_chi)))\n",
    "    print('Mother utterance count in {}: {}'.format(corpus, len(utt_mot)))\n",
    "    utt_chi_sum = utt_chi_sum + len(utt_chi)\n",
    "    utt_mot_sum = utt_mot_sum + len(utt_mot)\n",
    "\n",
    "utt_sum = utt_sum + utt_chi_sum + utt_mot_sum\n",
    "utt_chi_pc = round (utt_chi_sum/utt_sum*100, 2)  # Child utterance percentage\n",
    "utt_mot_pc = round (utt_mot_sum/utt_sum*100, 2)  # Mother utterance percentage\n",
    "\n",
    "# Print results\n",
    "print('\\n-------------------------------------------------\\n')\n",
    "print('\\nTotal child utterance count: {}'.format(utt_chi_sum))\n",
    "print('\\nTotal mother utterance count: {}'.format(utt_mot_sum))\n",
    "print('\\nTotal utterance count: {}'.format(utt_sum))\n",
    "print('\\nUtterance count percentage: {}% by child; {}% by mother'\n",
    "      .format(utt_chi_pc, utt_mot_pc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3152a07-b23c-4204-89e6-f20c1b191b97",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3 Data objects for further analysis\n",
    "\n",
    "The above code have created several data objects ready for further analysis, which are the `Reader`, `Token` and the `Utterance` objects. I will pickle these objects so that I don't need create these objects again every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5780e286-db36-4278-bb42-38e0cef73e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data = [search_result, data_idx]\n",
    "\n",
    "f = open('data/childes/corpus_info.pkl', 'wb')  \n",
    "pickle.dump(data, f, -1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ef5a21-10bc-41a5-aa55-900391885333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
