{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0361b10a-3d8d-4fdd-876a-dd275328f930",
   "metadata": {},
   "source": [
    "\n",
    "# Data curation\n",
    "\n",
    "Man Ho Wong | m.wong@pitt.edu | Feb 27th, 2022\n",
    "\n",
    "This notebook search for the datasets needed for this project in the following database:\n",
    "- [CHILDES](https://childes.talkbank.org/)  \n",
    "  *Reference:* MacWhinney, B. (2000). The CHILDES Project: Tools for analyzing talk. Third Edition. Mahwah, NJ: Lawrence Erlbaum Associates.\n",
    "\n",
    "I may not need datasets from [Wordbank](http://wordbank.stanford.edu/) as I found that CHILDES probably has all the data I need.\n",
    "\n",
    "I will also explore the datasets on the way to get a sense of the contents and the structures of the datasets (such as participant information, annotations, data format, etc.), as well as some basic statistics about the datasets. After that, I will identify the information I need in the datasets and compile the data for data processing later.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "577e6a35-45ca-404f-8027-8edb3678baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# data = pickle.load(open('data/childes/corpus_info.pkl', 'rb'))\n",
    "# search_result = data[0]\n",
    "# data_idx = data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2624715b-d6a5-41f6-b7bc-adf57127d855",
   "metadata": {},
   "source": [
    "\n",
    "# 1 Searching for suitable corpora in CHILDES\n",
    "\n",
    "CHILDES is a multilingual database containing corpora with transcriptions, audio recordings and/or video recordings of child speech and child-directed speech (CDS) at different developmental stages. Each corpus has a separate directory for each participant, and each directory contains the recording transcripts stored in CHAT formats. ([Example](https://childes.talkbank.org/access/Eng-NA/Brown.html))\n",
    "\n",
    "For this project, I will need to collect the transcipts for both the child speech and the associated CDS. Additionally, I will need the participant information (i.e. child age, sex and socioeconomic status (SES), mother's education) and some basic annotations of the words (i.e. morphemes and lexical categories). Participant information can be found in the header of each CHAT file as the metadata of the file. Annotation information can be found as dependent tiers embedded in the transcription.\n",
    "\n",
    "Let's take a quick look at a sample CHAT file first to see how the data is organized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98686cc-77f7-4f43-91dd-4575e7261f98",
   "metadata": {},
   "source": [
    "\n",
    "## 1.1 Acccessing data in CHAT file\n",
    "\n",
    "### Reading CHAT file\n",
    "\n",
    "The `PyLangAcq` package allows users to read CHAT files directly from a zip file. You can download and install it with the following code:  \n",
    "`$ pip install --upgrade pylangacq`\n",
    "\n",
    "For documentation, you can visit their [website](https://pylangacq.org/).\n",
    "\n",
    "I will use the Brown Corpus of CHILDES as an example below. The corpus has been downloaded from [here](https://childes.talkbank.org/data/Eng-NA/Brown.zip) and stored under `data_samples/childes/Brown.zip`. There are three folders in this corpus, each folder contains a dataset (a collection of CHAT files) for each child:\n",
    "\n",
    "```\n",
    "Brown.zip/  \n",
    "    |--Adam/  \n",
    "    |--Eve/  \n",
    "    |--Sarah/\n",
    "```\n",
    "\n",
    "I will use the `read_chat()` function of `PyLangAcq` to read all the CHAT files in the dataset `Adam`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce9e997b-a6eb-4ede-b8d2-eba6cc55bab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pylangacq.chat.Reader'>\n",
      "Number of CHAT files: 55\n"
     ]
    }
   ],
   "source": [
    "import pylangacq\n",
    "\n",
    "# Read CHAT files in the dataset 'Adam' in 'Brown.zip':\n",
    "path = 'data_samples/childes/Brown.zip'\n",
    "adam = pylangacq.read_chat(path, 'Adam')\n",
    "\n",
    "print(type(adam))\n",
    "print('Number of CHAT files:', adam.n_files())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda0575c-87d5-458f-915e-47a7737f4bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ages (year, month, day): [(2, 3, 4), (2, 3, 18), (2, 4, 3), (2, 4, 15), (2, 4, 30), (2, 5, 12), (2, 6, 3), (2, 6, 17), (2, 7, 1), (2, 7, 14), (2, 8, 1), (2, 8, 16), (2, 9, 4), (2, 9, 18), (2, 10, 2), (2, 10, 16), (2, 10, 30), (2, 11, 13), (2, 11, 28), (3, 0, 11), (3, 0, 25), (3, 1, 9), (3, 1, 26), (3, 2, 9), (3, 2, 21), (3, 3, 4), (3, 3, 18), (3, 4, 1), (3, 4, 18), (3, 5, 1), (3, 5, 15), (3, 5, 29), (3, 6, 9), (3, 7, 7), (3, 8, 1), (3, 8, 14), (3, 8, 26), (3, 9, 16), (3, 10, 15), (3, 11, 1), (3, 11, 14), (4, 0, 14), (4, 1, 15), (4, 2, 17), (4, 3, 9), (4, 4, 1), (4, 4, 13), (4, 5, 11), (4, 6, 24), (4, 7, 1), (4, 7, 29), (4, 9, 2), (4, 10, 2), (4, 10, 23), (5, 2, 12)]\n"
     ]
    }
   ],
   "source": [
    "# Ages when recordings were made\n",
    "print('Ages (year, month, day):', adam.ages())  # output: a list of tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4f47d0-163c-498c-81a2-7828c9270d7d",
   "metadata": {},
   "source": [
    "As shown above, `read_chat()` read the CHAT files and creates a `Reader` object. This is a `dataclass` storing data and metadata across all the CHAT files in `Adam`. You can access the data stored in the `Reader` by calling the appropriate methods/ attributes, such as `.n_files()` for number of CHAT files in the dataset. For example, `Adam` has 55 CHAT files. We can also get the ages when recordings were made by calling `.ages()`. Let's see what other information we can get from the `Reader` object in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb97d197-845e-46e4-a494-cd3765572cb0",
   "metadata": {},
   "source": [
    "### Accessing metadata stored in a CHAT file\n",
    "\n",
    "Metadata such as age range, date of recording, participants, etc. are stored in the header of each CHAT file. We can access such information by retrieving the `.header()` attribute. Here is the header for the first CHAT file in `adam`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caad807e-c699-4e50-9dfc-f9663e6c8f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UTF8': '',\n",
       " 'PID': '11312/c-00015632-1',\n",
       " 'Languages': ['eng'],\n",
       " 'Participants': {'CHI': {'name': 'Adam',\n",
       "   'language': 'eng',\n",
       "   'corpus': 'Brown',\n",
       "   'age': '2;03.04',\n",
       "   'sex': 'male',\n",
       "   'group': 'TD',\n",
       "   'ses': 'MC',\n",
       "   'role': 'Target_Child',\n",
       "   'education': '',\n",
       "   'custom': ''},\n",
       "  'MOT': {'name': 'Mother',\n",
       "   'language': 'eng',\n",
       "   'corpus': 'Brown',\n",
       "   'age': '',\n",
       "   'sex': 'female',\n",
       "   'group': '',\n",
       "   'ses': '',\n",
       "   'role': 'Mother',\n",
       "   'education': '',\n",
       "   'custom': ''},\n",
       "  'URS': {'name': 'Ursula_Bellugi',\n",
       "   'language': 'eng',\n",
       "   'corpus': 'Brown',\n",
       "   'age': '',\n",
       "   'sex': '',\n",
       "   'group': '',\n",
       "   'ses': '',\n",
       "   'role': 'Investigator',\n",
       "   'education': '',\n",
       "   'custom': ''},\n",
       "  'RIC': {'name': 'Richard_Cromer',\n",
       "   'language': 'eng',\n",
       "   'corpus': 'Brown',\n",
       "   'age': '',\n",
       "   'sex': '',\n",
       "   'group': '',\n",
       "   'ses': '',\n",
       "   'role': 'Investigator',\n",
       "   'education': '',\n",
       "   'custom': ''},\n",
       "  'COL': {'name': 'Colin_Fraser',\n",
       "   'language': 'eng',\n",
       "   'corpus': 'Brown',\n",
       "   'age': '',\n",
       "   'sex': '',\n",
       "   'group': '',\n",
       "   'ses': '',\n",
       "   'role': 'Investigator',\n",
       "   'education': '',\n",
       "   'custom': ''}},\n",
       " 'Date': {datetime.date(1962, 10, 8), datetime.date(1962, 10, 9)},\n",
       " 'Comment': 'Birth of CHI is 4-JUL-1960',\n",
       " 'Time Duration': '15:00-16:00',\n",
       " 'Types': 'long, toyplay, TD'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam.headers()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29beeb4-d948-4c36-b2f3-eab0a9095ffb",
   "metadata": {},
   "source": [
    "The output above, is a multilevel `dictionary`. To retrieve a specific piece of information we need, we can use the `dictionary` keys as usual.  \n",
    "Let's check if 'Adam' is a male as its biblical name suggests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4850e46-5e97-4e20-8a06-6aea94ef4090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam.headers()[0]['Participants']['CHI']['sex']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad9708f-391c-4edf-a245-25e4c2d06095",
   "metadata": {},
   "source": [
    "\n",
    "### Accessing annotations\n",
    "\n",
    "Next, I will check what kinds of annotation information are stored in each CHAT file. I will use the `.tokens()` method to access the tokens with annotation information. This method creates a `list` of `Token` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08d0375c-caed-42a4-9afc-d121b3f79e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(word='play', pos='n', mor='play', gra=Gra(dep=1, head=2, rel='MOD')),\n",
       " Token(word='checkers', pos='n', mor='checker-PL', gra=Gra(dep=2, head=0, rel='INCROOT')),\n",
       " Token(word='.', pos='.', mor='', gra=Gra(dep=3, head=2, rel='PUNCT')),\n",
       " Token(word='big', pos='adj', mor='big', gra=Gra(dep=1, head=2, rel='MOD')),\n",
       " Token(word='drum', pos='n', mor='drum', gra=Gra(dep=2, head=0, rel='INCROOT'))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = adam.tokens()\n",
    "tokens[:5]  # first five tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f07287-85f1-45ef-8234-a647f9a4fd24",
   "metadata": {},
   "source": [
    "Each `Token` is a `dataclass` with attributes (e.g. `word`,`pos`, etc.) as shown in the above example.  \n",
    "Annotations for each word are stored as the `Token`'s attributes (i.e. attributes other than `word`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2bf4834-9464-4b45-9c7e-3d8074646518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second token in 'Adam':\n",
      "Word: checkers\n",
      "Morpheme: checker-PL\n",
      "Part of speech: n\n"
     ]
    }
   ],
   "source": [
    "print(\"Second token in 'Adam':\")\n",
    "print('Word: {}\\nMorpheme: {}\\nPart of speech: {}'.format(\n",
    "    tokens[1].word, tokens[1].mor, tokens[1].pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a51c99-28b0-4d98-97ec-8a61817ab671",
   "metadata": {},
   "source": [
    "\n",
    "## 1.2 Search strategy\n",
    "\n",
    "Now that we know what kinds of information are stored in each CHAT file and how we can access them, we can start planning our search accordingly.\n",
    "\n",
    "### Identifying the scope of search\n",
    "\n",
    "There are dozens of English corpora in CHILDES. We don't need to download them all at once just to look for the corpora we need. `PyLangAcq` allows user to read a corpus directly with the corpus's URL. We can read the corpora one by one, and keep only the ones we need. To get the URLs for all the North American (NA) English corpora, one can use a web scraping tool to get all the links from the database's website and look for the corpus URLs from there. However, a much faster way is to take advantage of the TalkBank's [browsable database](https://sla.talkbank.org/TBB/childes):  \n",
    "1. navigate to CHILDES's North American English datasets (Eng-NA)\n",
    "2. copy the list of corpora directly to a spreadsheet program and save it as a `csv` file (example: `data_samples/childes/eng_NA_corpus_list.csv`)\n",
    "3. construct the URL for each corpus simply from the name of the corpus we get from step 2 (see below)\n",
    "\n",
    "CHILDES has a very well organized structure. Each corpus has the same URL format as follow:  \n",
    "`https://childes.talkbank.org/data/LANGUAGE/NAME_OF_CORPUS.zip`  \n",
    "For example, the URL for the Brown Corpus is: https://childes.talkbank.org/data/Eng-NA/Brown.zip\n",
    "\n",
    "Let's see what corpora will be included in our search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7132000-53ab-4b6b-b187-4f647de52636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 47 NA English corpora in CHILDES.\n",
      "Here are the first 10 corpora:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        Bates\n",
       "1    Bernstein\n",
       "2        Bliss\n",
       "3        Bloom\n",
       "4     Bohannon\n",
       "5    Braunwald\n",
       "6        Brent\n",
       "7        Brown\n",
       "8        Clark\n",
       "9    Demetras1\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the list of corpora into a Pandas Series:\n",
    "corpus_list = pd.read_csv('data/childes/eng_NA_corpus_list.csv', \n",
    "                          header=None, index_col=False, squeeze = True)\n",
    "\n",
    "print('There are {} NA English corpora in CHILDES.'.format(len(corpus_list)))\n",
    "print('Here are the first 10 corpora:')\n",
    "corpus_list.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d611aa-aa63-4fac-8306-a8e25464cd0a",
   "metadata": {},
   "source": [
    "There are 47 NA English corpora in CHILDES that we can potentially use! Since there is a huge volume of data that we have to evaluate, we need a good strategy to make the data search more efficient. For this project, I will develop a search stretegy similar to a data collection process following PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) statement ([flow diagram](https://view.officeapps.live.com/op/view.aspx?src=http%3A%2F%2Fwww.prisma-statement.org%2Fdocuments%2FPRISMA_2020_flow_diagram_new_SRs_v1.docx&wdOrigin=BROWSELINK), [publication](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1003583)), which is commonly used in medicine and life sciences for meta-analyses.\n",
    "\n",
    "### Searching data in three phases\n",
    "\n",
    "Instead of evaluating each of the CHAT files from all 47 corpora directly, I will perform the search in three phases, where each phase narrows down the scope of search. As the scope of search is becoming more manageable, I will use search criteria that are more specific. \n",
    "\n",
    "1. **Identification**  \n",
    "    Identify relevant corpora fitting a set of basic criteria  \n",
    "2. **Screening**  \n",
    "    Screen for CHAT files containing the information we need  \n",
    "3. **Refining**  \n",
    "    Refine the dataset by filtering CHAT files with more specific criteria\n",
    "\n",
    "## 1.3 Run the search\n",
    "\n",
    "### Phase 1: Identification\n",
    "\n",
    "I will first search for the corpora meeting the basic requirements of the project, i.e. corpora involving relevant participants (child for child speech analysis and mother for CDS analysis).\n",
    "\n",
    "Here are the search criteria:\n",
    "- Participants: data includes child or mother\n",
    "- Child information: data contains child age, sex and socioeconomic status (SES) information\n",
    "- Mother information: data contains socioeconomic status (SES), education information\n",
    "\n",
    "To access participant information, we can first read the data into a `Reader` object, and then retrieve the header information from the `.headers` attribute.\n",
    "\n",
    "#### About efficiency\n",
    "\n",
    "Reading all corpora at once into one `Reader` object is not memory-efficient, but reading each single CHAT file into a `Reader` involves a large number of iterations. Therefore, I will read and evaluate one corpus at a time. Once a CHAT file in the corpus fits the criteria, the rest of the CHAT files will not be evaluated and I will move on to the next corpus (because CHAT files belonging to the same corpus are supposed to come from the same study contain similar information).\n",
    "\n",
    "The `.headers` of a `Reader` contains a `list` of headers of *all* the CHAT files in the dataset.  \n",
    "Let's look at our favorite dataset 'Adam' again. Here are the headers of the first two CHAT files in 'Adam':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "887ed920-7dc7-403f-9e66-8e31753f6cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'UTF8': '',\n",
       "  'PID': '11312/c-00015632-1',\n",
       "  'Languages': ['eng'],\n",
       "  'Participants': {'CHI': {'name': 'Adam',\n",
       "    'language': 'eng',\n",
       "    'corpus': 'Brown',\n",
       "    'age': '2;03.04',\n",
       "    'sex': 'male',\n",
       "    'group': 'TD',\n",
       "    'ses': 'MC',\n",
       "    'role': 'Target_Child',\n",
       "    'education': '',\n",
       "    'custom': ''},\n",
       "   'MOT': {'name': 'Mother',\n",
       "    'language': 'eng',\n",
       "    'corpus': 'Brown',\n",
       "    'age': '',\n",
       "    'sex': 'female',\n",
       "    'group': '',\n",
       "    'ses': '',\n",
       "    'role': 'Mother',\n",
       "    'education': '',\n",
       "    'custom': ''},\n",
       "   'URS': {'name': 'Ursula_Bellugi',\n",
       "    'language': 'eng',\n",
       "    'corpus': 'Brown',\n",
       "    'age': '',\n",
       "    'sex': '',\n",
       "    'group': '',\n",
       "    'ses': '',\n",
       "    'role': 'Investigator',\n",
       "    'education': '',\n",
       "    'custom': ''},\n",
       "   'RIC': {'name': 'Richard_Cromer',\n",
       "    'language': 'eng',\n",
       "    'corpus': 'Brown',\n",
       "    'age': '',\n",
       "    'sex': '',\n",
       "    'group': '',\n",
       "    'ses': '',\n",
       "    'role': 'Investigator',\n",
       "    'education': '',\n",
       "    'custom': ''},\n",
       "   'COL': {'name': 'Colin_Fraser',\n",
       "    'language': 'eng',\n",
       "    'corpus': 'Brown',\n",
       "    'age': '',\n",
       "    'sex': '',\n",
       "    'group': '',\n",
       "    'ses': '',\n",
       "    'role': 'Investigator',\n",
       "    'education': '',\n",
       "    'custom': ''}},\n",
       "  'Date': {datetime.date(1962, 10, 8), datetime.date(1962, 10, 9)},\n",
       "  'Comment': 'Birth of CHI is 4-JUL-1960',\n",
       "  'Time Duration': '15:00-16:00',\n",
       "  'Types': 'long, toyplay, TD'},\n",
       " {'UTF8': '',\n",
       "  'PID': '11312/c-00015633-1',\n",
       "  'Languages': ['eng'],\n",
       "  'Participants': {'CHI': {'name': 'Adam',\n",
       "    'language': 'eng',\n",
       "    'corpus': 'Brown',\n",
       "    'age': '2;03.18',\n",
       "    'sex': 'male',\n",
       "    'group': 'TD',\n",
       "    'ses': 'MC',\n",
       "    'role': 'Target_Child',\n",
       "    'education': '',\n",
       "    'custom': ''},\n",
       "   'MOT': {'name': 'Mother',\n",
       "    'language': 'eng',\n",
       "    'corpus': 'Brown',\n",
       "    'age': '',\n",
       "    'sex': 'female',\n",
       "    'group': '',\n",
       "    'ses': '',\n",
       "    'role': 'Mother',\n",
       "    'education': '',\n",
       "    'custom': ''},\n",
       "   'URS': {'name': 'Ursula_Bellugi',\n",
       "    'language': 'eng',\n",
       "    'corpus': 'Brown',\n",
       "    'age': '',\n",
       "    'sex': '',\n",
       "    'group': '',\n",
       "    'ses': '',\n",
       "    'role': 'Investigator',\n",
       "    'education': '',\n",
       "    'custom': ''},\n",
       "   'RIC': {'name': 'Richard_Cromer',\n",
       "    'language': 'eng',\n",
       "    'corpus': 'Brown',\n",
       "    'age': '',\n",
       "    'sex': '',\n",
       "    'group': '',\n",
       "    'ses': '',\n",
       "    'role': 'Investigator',\n",
       "    'education': '',\n",
       "    'custom': ''}},\n",
       "  'Date': {datetime.date(1962, 10, 22), datetime.date(1962, 10, 23)},\n",
       "  'Time Duration': '15:00-16:00',\n",
       "  'Types': 'long, toyplay, TD',\n",
       "  'Tape Location': '646'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam.headers()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd773f94-eed8-47db-8854-6cb8a34371be",
   "metadata": {},
   "source": [
    "Each header is a multi-level `dictionary`. To access the participant information in each of the headers, one can loop through each header and index into the sub-level dictionary associated with the key `Participants`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a27d55a-6088-4471-8088-88ca43909070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one file involves mother as a participant in the dataset \"Adam\"!\n"
     ]
    }
   ],
   "source": [
    "for h in adam.headers():\n",
    "    if 'MOT' in h ['Participants']:  # search criterion\n",
    "        print('At least one file involves mother as a participant in the dataset \"Adam\"!')\n",
    "        break  # Break the loop once the search criterion is met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d832f90f-f18b-441e-871b-67cccf3dd7c1",
   "metadata": {},
   "source": [
    "Let's begin the first phase of the search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37304e38-1ef1-4aa1-ad4e-9fee5b458318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 47/47 [06:27<00:00,  8.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search completed! 13 corpora found!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "search_result = []  # To store a list of corpora matching the criteria\n",
    "\n",
    "# Search each corpus in the list:\n",
    "for corpus_name in tqdm(corpus_list):  # tqdm for progress bar\n",
    "    \n",
    "    # Download URL for the corpus:\n",
    "    corpus_url = 'https://childes.talkbank.org/data/Eng-NA/'+corpus_name+'.zip'\n",
    "    \n",
    "    # read the corpus into a Reader object:\n",
    "    corpus = pylangacq.read_chat(corpus_url)\n",
    "   \n",
    "    # Search criteria:  \n",
    "    # - Child ('CHI') or mother ('MOT') is included as participant.\n",
    "    # - Info about Child's SES or mother's SES/education is provided.\n",
    "    # Note: Check if 'CHI' or 'MOT' is present before checking SES/education info\n",
    "    #       (if the condition before 'and' is False, the second condition won't\n",
    "    #       be evaluated. This is not only more efficient, but also prevents\n",
    "    #       error when evaluating the second condition if 'CHI'/'MOT' is absent).\n",
    "    \n",
    "    for h in corpus.headers():\n",
    "        if (\n",
    "            (('CHI' in h['Participants']) and (h['Participants']['CHI']['ses'] != '')) or \n",
    "            (('MOT' in h['Participants']) and (h['Participants']['MOT']['ses'] != '')) or \n",
    "            (('MOT' in h['Participants']) and (h['Participants']['MOT']['education'] != ''))\n",
    "        ):\n",
    "            search_result.append(corpus_name)  # store corpus name in 'search_result'\n",
    "            break  # Break the for loop and move on to the next corpus\n",
    "\n",
    "print('Search completed! {} corpora found!'.format(len(search_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ab9831-c39c-42eb-a9eb-5ce182450676",
   "metadata": {},
   "source": [
    "Let's see which corpora contain the data we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a281639-489c-4578-86dc-6a779ac693e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13 corpora matching the criteria:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Bates',\n",
       " 'Bernstein',\n",
       " 'Brown',\n",
       " 'Clark',\n",
       " 'Demetras2',\n",
       " 'Gleason',\n",
       " 'HSLLD',\n",
       " 'Hall',\n",
       " 'Hicks',\n",
       " 'Nelson',\n",
       " 'NewmanRatner',\n",
       " 'Post',\n",
       " 'VanHouten']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\n{} corpora matching the criteria:'.format(len(search_result)))\n",
    "search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90501fb-2470-4586-9544-7c5efcbdff7e",
   "metadata": {},
   "source": [
    "Nice! We have narrowed down the number of corpora we need to process from 47 to 13.  \n",
    "Next, I will download the zip files for the corpora to my local drive and extract them to the folder `data/childes/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1802310c-4a91-455b-8873-4b9ad0caa838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Where zip files will be stored locally\n",
    "target_dir = 'data/childes/corpus_zip/'\n",
    "\n",
    "# Create a folder to store the zip files\n",
    "os.mkdir(target_dir)\n",
    "\n",
    "for corpus_name in search_result:\n",
    "    \n",
    "    print('Downloading and extracting {}...'.format(corpus_name))\n",
    "    \n",
    "    # Corpus's download URL:\n",
    "    corpus_url = 'https://childes.talkbank.org/data/Eng-NA/'+corpus_name+'.zip'\n",
    "    \n",
    "    # Path in local drive:\n",
    "    zip_path = target_dir + corpus_name + '.zip'\n",
    "    \n",
    "    # Download corpus from URL\n",
    "    urllib.request.urlretrieve(corpus_url, zip_path)\n",
    "    \n",
    "    # Extract zip file\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        z.extractall('data/childes')\n",
    "        \n",
    "print('Done! Zip files were stored in {} \\nand extracted to {}.'\n",
    "      .format(target_dir, 'data/childes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d14ea2-e6d3-43b2-b0f6-59a6a5639444",
   "metadata": {},
   "source": [
    "### Phase 2: Screening\n",
    "\n",
    "We have downloaded all the corpora containing any number of CHAT files matching our search criteria. Since not all the files in the same corpus match the search criteria, I will exclude those files which do not match the criteria from my analysis. I will create a `DataFrame` as an index pointing to the files that meet the search criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56142f20-ba10-4a52-b41c-2cd2404300c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_list = []\n",
    "corpus_name_list = []\n",
    "chi_name_list = []\n",
    "chi_age_d_list = []\n",
    "chi_age_m_list = []\n",
    "chi_sex_list = []\n",
    "chi_group_list = []\n",
    "chi_ses_list = []\n",
    "mot_edu_list = []\n",
    "\n",
    "for corpus_name in tqdm(search_result):  # tqdm for progress bar\n",
    "    corpus = pylangacq.Reader.from_dir('data/childes/'+corpus_name)\n",
    "    \n",
    "    for f in corpus:\n",
    "        \n",
    "        if (\n",
    "            # Participants must include both the child and the mother.\n",
    "            (('CHI' in f.headers()[0]['Participants']) and \n",
    "            ('MOT' in f.headers()[0]['Participants'])) \n",
    "            \n",
    "            and\n",
    "            \n",
    "            # Dataset should include child's or mother's SES/education.\n",
    "            (( 'ses' in f.headers()[0]['Participants']['MOT']) or\n",
    "             (f.headers()[0]['Participants']['CHI']['ses'] != '') or\n",
    "             (f.headers()[0]['Participants']['MOT']['education'] != ''))\n",
    "        ):\n",
    "            \n",
    "            age_raw     = f.ages()[0]  # age in raw format\n",
    "            chi_age_d   = (age_raw[0]*12 + age_raw[1])*30 + age_raw[2]\n",
    "            chi_age_m   = age_raw[0]*12 + age_raw[1] + round(age_raw[2]/30,1)\n",
    "\n",
    "            # Include only children <= 72 months:\n",
    "            if chi_age_m <= 72: \n",
    "            \n",
    "                file_path   = f.file_paths()[0].replace('\\\\','/')\n",
    "                file_path   = file_path[0].lower() + file_path[1:]\n",
    "                \n",
    "                corpus_name = f.headers()[0]['Participants']['CHI']['corpus']\n",
    "                chi_name    = f.headers()[0]['Participants']['CHI']['name']\n",
    "                chi_sex     = f.headers()[0]['Participants']['CHI']['sex']\n",
    "                chi_group   = f.headers()[0]['Participants']['CHI']['group']\n",
    "                chi_ses     = f.headers()[0]['Participants']['CHI']['ses']\n",
    "                mot_edu     = f.headers()[0]['Participants']['MOT']['education']\n",
    "\n",
    "                file_path_list.append(file_path) \n",
    "                corpus_name_list.append(corpus_name)\n",
    "                chi_name_list.append(chi_name)\n",
    "                chi_age_d_list.append(chi_age_d)\n",
    "                chi_age_m_list.append(chi_age_m)\n",
    "                chi_sex_list.append(chi_sex)\n",
    "                chi_group_list.append(chi_group)\n",
    "                chi_ses_list.append(chi_ses)\n",
    "                mot_edu_list.append(mot_edu)\n",
    "\n",
    "data_idx = pd.DataFrame({'file_path':file_path_list,\n",
    "                         'corpus':corpus_name_list,\n",
    "                         'name':chi_name_list,\n",
    "                         'age_d':chi_age_d_list,\n",
    "                         'age_m':chi_age_m_list,\n",
    "                         'sex':chi_sex_list,\n",
    "                         'group':chi_group_list,\n",
    "                         'ses':chi_ses_list,\n",
    "                         'mot_edu':mot_edu_list,\n",
    "                        })\n",
    "\n",
    "print('Done!')\n",
    "data_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018c1218-94b1-40f9-9686-01581f8e2b53",
   "metadata": {},
   "source": [
    "### Phase 3: Refining\n",
    "\n",
    "This will be done on the fly during data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c32566a-800e-43a8-864f-855a6b09f2f1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2 Basic statistics\n",
    "\n",
    "## 2.1 Token count\n",
    "\n",
    "We can use the methods `.tokens()` and `.utterances` to access the token and utterance information stored in the `Reader` objects. Let's see how many tokens are three in these corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46d248c-04bb-42c7-a810-8d36815fa793",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []  # To store a list of lists of tokens for all corpora\n",
    "token_sum = 0    # Total token counter\n",
    "\n",
    "for corpus in search_result:\n",
    "    files = data_idx[data_idx.corpus==corpus].file_path\n",
    "    reader = pylangacq.Reader.from_files(files)\n",
    "    # Get token info and store them in 'all_tokens':\n",
    "    tokens = reader.tokens()  # list of Token objects\n",
    "    all_tokens.append(tokens)\n",
    "    \n",
    "    # Print result\n",
    "    print('Token count in {}: {}'.format(corpus, len(tokens)))\n",
    "    token_sum = token_sum + len(tokens)\n",
    "\n",
    "# Print result\n",
    "print('\\nTotal token count: {}'.format(token_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0b12ae-0bd9-4a95-a4dc-df10a680b278",
   "metadata": {},
   "source": [
    "\n",
    "## 2.2 Utterance count\n",
    "\n",
    "Similarly, we can access the utterance infomation stored in each `Reader` object. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd72aae1-ce61-45ba-8964-ca244bd93151",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = data_idx.file_path[0]  # first file in 'data_idx'\n",
    "reader = pylangacq.Reader.from_files([file])\n",
    "reader.utterances()[3]  # 4th utterance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542b09fa-850d-4df5-9342-0ade51380155",
   "metadata": {},
   "source": [
    "The example above shows the second utterance and their annotation information in the first corpus ('Bates'), including the words, the speaker and more. To look at child-directed speech (CDS) specifically, we can set the `.utterances()`'s `participants` option to `MOT`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a37dcdd-95ff-4f19-9b9f-9799b10ece16",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.utterances(participants='MOT')[0] # first utterance by mother"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ae966-1f9e-4183-94d4-1f9e60603afb",
   "metadata": {},
   "source": [
    "Next, I will look at how many utterances are there in the data. How many utterances are there in the child speech, and how many in the mother's CDS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e9851-9ef7-4461-934a-55fc1777d470",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_utt_chi = []  # To store a list of lists of child utterances for all corpora\n",
    "all_utt_mot = []  # To store a list of lists of mother utterances for all corpora\n",
    "utt_chi_sum = 0   # Total child utterance counter\n",
    "utt_mot_sum = 0   # Total mother utterance counter\n",
    "utt_sum = 0       # Total utterance counter\n",
    "\n",
    "for corpus in search_result:\n",
    "    files = data_idx[data_idx.corpus==corpus].file_path\n",
    "    reader = pylangacq.Reader.from_files(files)\n",
    "    # Get utterances and store them in 'all_utt_chi' or 'all_utt_mot':\n",
    "    utt_chi = reader.utterances(participants='CHI')  # list of Utterance object\n",
    "    utt_mot = reader.utterances(participants='MOT')  # list of Utterance object\n",
    "    all_utt_chi.append(utt_chi)\n",
    "    all_utt_mot.append(utt_mot)\n",
    "    \n",
    "    # Print results\n",
    "    print('Child utterance count in {}: {}'.format(corpus, len(utt_chi)))\n",
    "    print('Mother utterance count in {}: {}'.format(corpus, len(utt_mot)))\n",
    "    utt_chi_sum = utt_chi_sum + len(utt_chi)\n",
    "    utt_mot_sum = utt_mot_sum + len(utt_mot)\n",
    "\n",
    "utt_sum = utt_sum + utt_chi_sum + utt_mot_sum\n",
    "utt_chi_pc = round (utt_chi_sum/utt_sum*100, 2)  # Child utterance percentage\n",
    "utt_mot_pc = round (utt_mot_sum/utt_sum*100, 2)  # Mother utterance percentage\n",
    "\n",
    "# Print results\n",
    "print('\\n-------------------------------------------------\\n')\n",
    "print('\\nTotal child utterance count: {}'.format(utt_chi_sum))\n",
    "print('\\nTotal mother utterance count: {}'.format(utt_mot_sum))\n",
    "print('\\nTotal utterance count: {}'.format(utt_sum))\n",
    "print('\\nUtterance count percentage: {}% by child; {}% by mother'\n",
    "      .format(utt_chi_pc, utt_mot_pc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe28b5d-c476-4578-93b5-4e0cb9392665",
   "metadata": {},
   "source": [
    "## 2.3 Demographics of participants\n",
    "\n",
    "Vocabulary development can be influenced by a participant's background, such as age, sex, SES, mother's education and developmental groups (e.g. typically developing children). Let's look at these factors first to get a better idea of how the demographics looks like and what kinds of information are available in the data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a3ec2-8622-445a-986d-a54fca521083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot') # ggplot style\n",
    "\n",
    "\n",
    "ses_count = data_idx.groupby('ses').aggregate('count').file_path[1:]\n",
    "\n",
    "plt.pie(ses_count, labels=ses_count.index, autopct='%1.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c1ecd-5881-481e-89cf-fc5792799e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_idx.groupby('ses').aggregate('count').file_path[1:].i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3152a07-b23c-4204-89e6-f20c1b191b97",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3 Data objects for further analysis\n",
    "\n",
    "The above code have created several data objects ready for further analysis. I will pickle these objects so that I don't need create these objects again every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5780e286-db36-4278-bb42-38e0cef73e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data = [search_result, data_idx2]\n",
    "\n",
    "f = open('data/childes/corpus_info.pkl', 'wb')  \n",
    "pickle.dump(data, f, -1)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
